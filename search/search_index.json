{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Zillion is a free, open data warehousing and dimensional modeling tool that allows combining and analyzing data from multiple datasources through a simple API. It writes SQL so you don't have to, and it easily bolts onto existing database infrastructure via SQLAlchemy. With Zillion you can: Define a warehouse that contains a variety of SQL and/or file-like datasources Define or reflect metrics, dimensions, and relationships in your data Run multi-datasource reports and combine the results in a DataFrame Flexibly aggregate your data with multi-level rollups and table pivots Customize or combine fields with formulas Apply technical transformations including rolling, cumulative, and rank statistics Apply automatic type conversions - i.e. get a \"year\" dimension for free from a \"date\" column Save and share report specifications Utilize ad hoc or public datasources, tables, and fields to enrich reports Installation \u00b6 \u26a0\ufe0f Warning : This project is in an alpha state and is rapidly changing. Please test carefully for production usage and report any issues. $ pip install zillion Primer \u00b6 The following is meant to give a quick overview of some theory and nomenclature used in data warehousing with Zillion . Skip below for a quickstart example . In short: Zillion writes SQL for you and makes data accessible through a very simple API: result = warehouse . execute( metrics = [ \"revenue\" , \"leads\" ], dimensions = [ \"date\" ], criteria = [ ( \"date\" , \">\" , \"2020-01-01\" ), ( \"partner\" , \"=\" , \"Partner A\" ) ] ) Metrics and Dimensions \u00b6 In Zillion there are two main types of Fields that will be used in your report requests: Dimensions : attributes of data used for labelling, grouping, and filtering Metrics : facts and measures that may be broken down along dimensions A Field encapsulates the concept of a column in your data. For example, you may have a Field called \"revenue\". That Field may occur across several datasources or possibly in multiple tables within a single datasource. Zillion understands that all of those columns represent the same concept, and it can try to use any of them to satisfy reports requesting \"revenue\". Likewise there are two main types of tables used to structure your warehouse: Dimension Tables : reference/attribute tables containing only related dimensions Metric Tables : fact tables that may contain metrics and some related dimensions/attributes Dimension tables are often static or slowly growing in terms of row count and contain attributes tied to a primary key. Some common examples would be lists of US Zip Codes or company/partner directories. Metric tables are generally more transactional in nature. Some common examples would be records for web requests, ecommerce sales, or stock market price history. Warehouse Theory \u00b6 If you really want to go deep on dimensional modeling and the drill-across querying technique Zillion employs, I recommend reading Ralph Kimball's book on data warehousing. To summarize, drill-across querying forms one or more queries to satisfy a report request for metrics that may exist across multiple datasources and/or tables at a particular dimension grain. Zillion technically supports snowflake or star schemas, though it isn't picky about it. You can specify table relationships through a parent-child lineage, and Zillion can also infer acceptable joins based on the presence of dimension table primary keys. Query Layers \u00b6 Zillion reports can be thought of as running in two layers: DataSource Layer : SQL queries against the warehouse's datasources Combined Layer : A final SQL query against the combined data from the DataSource Layer The Combined Layer is just another SQL database (in-memory SQLite by default) that is used to tie the datasource data together and apply a few additional features such as rollups, row filters, row limits, sorting, pivots, and technical computations. Executing Reports \u00b6 The main purpose of Zillion is to execute reports against a Warehouse . You'll see how to initialize a Warehouse in a bit, but at a high level you will be crafting reports as follows: result = warehouse . execute( metrics = [ \"revenue\" , \"leads\" ], dimensions = [ \"date\" ], criteria = [ ( \"date\" , \">\" , \"2020-01-01\" ), ( \"partner\" , \"=\" , \"Partner A\" ) ] ) print(result . df) # Pandas DataFrame When comparing to writing SQL, it's helpful to think of the dimensions as the target columns of a group by SQL statement. Think of the metrics as the columns you are aggregating . Think of the criteria as the where clause . Your criteria are applied in the DataSource Layer SQL queries. The ReportResult has a Pandas DataFrame with the dimensions as the index and the metrics as the columns. A Report is said to have a grain , which defines the dimensions each metric must be able to join to in order to satisfy the Report requirements. The grain is a combination of all dimensions, including those referenced in criteria or in metric formulas. In the example above, the grain would be {date, partner} . Both \"revenue\" and \"leads\" must be able to join to those dimensions for this report to be possible. These concepts can take time to sink in and obviously vary with the specifics of your data model, but you will become more familiar with them as you start putting together reports against your data warehouses. Example - Sales Analytics \u00b6 Below we will walk through a simple hypothetical sales data model that demonstrates basic DataSource and Warehouse configuration and then shows some sample reports . The data is a simple SQLite database that is part of the Zillion test code. For reference, the schema is as follows: CREATE TABLE partners ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL UNIQUE , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE campaigns ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL UNIQUE , category VARCHAR NOT NULL , partner_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE leads ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL , campaign_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE sales ( id INTEGER PRIMARY KEY , item VARCHAR NOT NULL , quantity INTEGER NOT NULL , revenue DECIMAL( 10 , 2 ), lead_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); Warehouse Creation \u00b6 A Warehouse may be created from an existing SQLAlchemy MetaData instance, purely from a JSON/YAML configuration, or a combination of the two. The code below shows how it can be done in as little as one line of code if you have a pointer to a JSON/YAML Warehouse config. Note : Defining your config in JSON or YAML is recommended, so we'll save an example of defining Zillion metadata directly on your SQLAlchemy objects for another time. from zillion import Warehouse wh = Warehouse(config = \"https://raw.githubusercontent.com/totalhack/zillion/master/examples/example_wh_config.json\" ) This example config uses a data_url in its DataSource connect info that tells Zillion to dynamically download that data and connect to it as a SQLite database. This is useful for quick examples or analysis, though in most scenarios you would put a connection string to an existing database like you see here The basics of Zillion's configuration structure are as follows: A Warehouse config has the following main sections: metrics : optional list of metric configs for global metrics dimensions : optional list of dimension configs for global dimensions datasources : mapping of datasource names to datasource configs or config URLs A DataSource config has the following main sections: connect : database connection url or dict of connect params metrics : optional list of metric configs specific to this datasource dimensions : optional list of dimension configs specific to this datasource tables : mapping of table names to table configs or config URLs Tip: datasource and table configs may also be replaced with a URL that points to a local or remote config file. In this example all four tables in our database are included in the config, two as dimension tables and two as metric tables. The tables are linked through a parent->child relationship: partners to campaigns, and leads to sales. Some tables also utilize the create_fields flag to automatically create Fields on the datasource from column definitions. Other metrics and dimensions are defined explicitly. To view the structure of this Warehouse after init you can use the print_info method which shows all metrics, dimensions, tables, and columns that are part of your data warehouse: wh . print_info() # Formatted print of the Warehouse structure For a deeper dive of the config schema please see the full docs . Reports \u00b6 Example: Get sales, leads, and revenue by partner: result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" ] ) print(result . df) \"\"\" sales leads revenue partner_name Partner A 11 4 165.0 Partner B 2 2 19.0 Partner C 5 1 118.5 \"\"\" Example: Let's limit to Partner A and break down by its campaigns: result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"campaign_name\" ], criteria = [( \"partner_name\" , \"=\" , \"Partner A\" )] ) print(result . df) \"\"\" sales leads revenue campaign_name Campaign 1A 5 2 83 Campaign 2A 6 2 82 \"\"\" Example: The output below shows rollups at the campaign level within each partner, and also a rollup of totals at the partner and campaign level. Note: the output contains a special character to mark DataFrame rollup rows that were added to the result. The ReportResult object contains some helper attributes to automatically access or filter rollups, as well as a df_display attribute that returns the result with friendlier display values substituted for special characters. The under-the-hood special character is left here for illustration, but may not render the same in all scenarios. from zillion import RollupTypes result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" , \"campaign_name\" ], rollup = RollupTypes . ALL ) print(result . df) \"\"\" sales leads revenue partner_name campaign_name Partner A Campaign 1A 5.0 2.0 83.0 Campaign 2A 6.0 2.0 82.0 \udbff\udfff 11.0 4.0 165.0 Partner B Campaign 1B 1.0 1.0 6.0 Campaign 2B 1.0 1.0 13.0 \udbff\udfff 2.0 2.0 19.0 Partner C Campaign 1C 5.0 1.0 118.5 \udbff\udfff 5.0 1.0 118.5 \udbff\udfff \udbff\udfff 18.0 7.0 302.5 \"\"\" See the Report docs for more information on supported rollup behavior. Example: Save a report spec (not the data): First you must make sure you have saved your Warehouse , as saved reports are scoped to a particular Warehouse ID. To save a Warehouse you must provide a URL that points to the complete config. name = \"My Unique Warehouse Name\" config_url = < some url pointing to a complete warehouse config > wh . save(name, config_url) # wh.id is populated after this spec_id = wh . save_report( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" ] ) Note : If you built your Warehouse in python from a list of DataSources , or passed in a dict for the config param on init, there currently is not a built-in way to output a complete config to a file for reference when saving. Example: Load and run a report from a spec ID: result = wh . execute_id(spec_id) Note: The ZILLION_CONFIG environment var can point to a yaml config file. The database used to store Zillion report specs can be configured by setting the DB_URL value in your Zillion config to a valid database connection string. By default a SQLite DB in /tmp is used. See this sample config . Environment vars prefixed with ZILLION_ can override config settings (i.e. ZILLION_DB_URL will override DB_URL). Example: Unsupported Grain If you attempt an impossible report, you will get an UnsupportedGrainException . The report below is impossible because it attempts to break down the leads metric by a dimension that only exists in a child table. Generally speaking, child tables can join back up to parents (and \"siblings\" of parents) to find dimensions, but not the other way around. # Fails with UnsupportedGrainException result = wh . execute( metrics = [ \"leads\" ], dimensions = [ \"sale_id\" ] ) Advanced Topics \u00b6 Formula Metrics \u00b6 In our example above our config included a formula-based metric called \"rpl\", which is simply revenue / leads . A FormulaMetric combines other metrics and/or dimensions to calculate a new metric at the Combined Layer of querying. The syntax must match your Combined Layer database, which is SQLite in our example. { \"name\" : \"rpl\" , \"aggregation\" : \"mean\" , \"rounding\" : 2 , \"formula\" : \"{revenue}/{leads}\" } DataSource Formulas \u00b6 Our example also includes a metric \"sales\" whose value is calculated via formula at the DataSource Layer of querying. Note the following in the fields list for the \"id\" param in the \"main.sales\" table. These formulas are in the syntax of the particular DataSource database technology, which also happens to be SQLite in our example. \"fields\" : [ \"sale_id\" , { \"name\" : \"sales\" , \"ds_formula\" : \"COUNT(DISTINCT sales.id)\" } ] Type Conversions \u00b6 Our example also automatically created a handful of dimensions from the \"created_at\" columns of the leads and sales tables. Support for automatic type conversions is limited, but for date/datetime columns in supported DataSource technologies you can get a variety of dimensions for free this way. The output of wh.print_info will show the added dimensions, which are prefixed with \"lead_\" or \"sale_\" as specified by the optional type_conversion_prefix in the config for each table. Some examples of auto-generated dimensions in our example warehouse include sale_hour, sale_day_name, sale_month, sale_year, etc. As an optimization in the where clause of underlying report queries, Zillion will try to apply conversions to criteria values instead of columns. For example, it is generally more efficient to query as my_datetime > '2020-01-01' and my_datetime < '2020-01-02' instead of DATE(my_datetime) == '2020-01-01' , because the latter can prevent index usage in many database technologies. The ability to apply conversions to values instead of columns varies by field and DataSource technology as well. To prevent type conversions, set skip_conversion_fields to true on your DataSource config. See zillion.field.TYPE_ALLOWED_CONVERSIONS and zillion.field.DIALECT_CONVERSIONS for more details on currently supported conversions. Config Variables \u00b6 If you'd like to avoid putting sensitive connection information directly in your DataSource configs you can leverage config variables. In your Zillion yaml config you can specify a DATASOURCE_CONTEXTS section as follows: DATASOURCE_CONTEXTS : my_ds_name : user : user123 pass : goodpassword host : 127.0.0.1 schema : reporting Then when your DataSource config for the datasource named \"my_ds_name\" is read, it can use this context to populate variables in your connection url: \"datasources\" : { \"my_ds_name\" : { \"connect\" : \"mysql+pymysql://{user}:{pass}@{host}/{schema}\" ... } } DataSource Priority \u00b6 On Warehouse init you can specify a default priority order for datasources by name. This will come into play when a report could be satisfied by multiple datasources. DataSources earlier in the list will be higher priority. This would be useful if you wanted to favor a set of faster, aggregate tables that are grouped in a DataSource . wh = Warehouse(config = config, ds_priority = [ \"aggr_ds\" , \"raw_ds\" , ... ]) Ad Hoc Metrics \u00b6 You may also define metrics \"ad hoc\" with each report request. Below is an example that creates a revenue-per-lead metric on the fly. These only exist within the scope of the report, and the name can not conflict with any existing fields: result = wh . execute( metrics = [ \"leads\" , { \"formula\" : \"{revenue}/{leads}\" , \"name\" : \"my_rpl\" } ], dimensions = [ \"partner_name\" ] ) Ad Hoc Tables \u00b6 Zillion also supports creation or syncing of ad hoc tables in your database during DataSource or Warehouse init. An example of a table config that does this is shown here . It uses the table config's data_url and if_exists params to control the syncing and/or creation of the \"main.dma_zip\" table from a remote CSV in a SQLite database. The same can be done in other database types too. The potential performance drawbacks to such an approach should be obvious, particularly if you are initializing your warehouse often or if the remote data file is large. It is often better to sync and create your data ahead of time so you have complete schema control, but this method can be very useful in certain scenarios. \u26a0\ufe0f Warning : be careful not to overwrite existing tables in your database! Technicals \u00b6 There are a variety of technical computations that can be applied to metrics to compute rolling, cumulative, or rank statistics. For example, to compute a 5-point moving average on revenue one might define a new metric as follows: { \"name\" : \"revenue_ma_5\" , \"type\" : \"numeric(10,2)\" , \"aggregation\" : \"sum\" , \"rounding\" : 2 , \"technical\" : \"mean(5)\" } Technical computations are computed at the Combined Layer, whereas the \"aggregation\" is done at the DataSource Layer (hence needing to define both above). For more info on how shorthand technical strings are parsed, see the parse_technical_string code. For a full list of supported technical types see zillion.core.TechnicalTypes . Technicals also support two modes: \"group\" and \"all\". The mode controls how to apply the technical computation across the data's dimensions. In \"group\" mode, it computes the technical across the last dimension, whereas in \"all\" mode in computes the technical across all data without any regard for dimensions. The point of this becomes more clear if you try to do a \"cumsum\" technical across data broken down by something like [\"partner_name\", \"date\"]. If \"group\" mode is used (the default in most cases) it will do cumulative sums within each partner over the date ranges. If \"all\" mode is used, it will do a cumulative sum across every data row. You can be explicit about the mode by appending it to the technical string: i.e. \"cumsum:all\" or \"mean(5):group\" Supported DataSources \u00b6 Zillion's goal is to support any database technology that SQLAlchemy supports. That said the support and testing levels in Zillion vary at the moment. In particular, the ability to do type conversions, database reflection, and kill running queries all require some database-specific code for support. The following list summarizes known support levels. Your mileage may vary with untested database technologies that SQLAlchemy supports (it might work just fine, just hasn't been tested yet). Please report bugs and help add more support! SQLite: supported and tested MySQL: supported and tested PostgreSQL: supported and lightly tested MSSQL: not tested, but support is planned Oracle: not tested, but support is planned BigQuery, Redshift, Snowflake, etc: not tested, but support is planned Note that this is different than the database support for the Combined Layer database. Currently only SQLite is supported there, though it is planned to make this more generic such that any SQLAlchemy supported database could be used. Multiprocess Considerations \u00b6 If you plan to run Zillion in a multiprocess scenario, whether on a single node or across multiple nodes, there are a couple of things to consider: SQLite DataSources do not scale well and may run into locking issues with multiple processes trying to access them on the same node. Any file-based database technology that isn't centrally accessible would be challenging when using multiple nodes. Ad Hoc DataSource and Ad Hoc Table downloads should be avoided as they may conflict/repeat across each process. Offload this to an external ETL process that is better suited to manage those data flows in a scalable production scenario. Note that you can still use the default SQLite in-memory Combined Layer DB without issues, as that is made on the fly with each report request and requires no coordination/communication with other processes or nodes. Related Projects \u00b6 Awesome Zillion - A collection of Zillion projects and resources. Zillion Web UI - A demo UI and web API for Zillion","title":"Getting Started"},{"location":"#introduction","text":"Zillion is a free, open data warehousing and dimensional modeling tool that allows combining and analyzing data from multiple datasources through a simple API. It writes SQL so you don't have to, and it easily bolts onto existing database infrastructure via SQLAlchemy. With Zillion you can: Define a warehouse that contains a variety of SQL and/or file-like datasources Define or reflect metrics, dimensions, and relationships in your data Run multi-datasource reports and combine the results in a DataFrame Flexibly aggregate your data with multi-level rollups and table pivots Customize or combine fields with formulas Apply technical transformations including rolling, cumulative, and rank statistics Apply automatic type conversions - i.e. get a \"year\" dimension for free from a \"date\" column Save and share report specifications Utilize ad hoc or public datasources, tables, and fields to enrich reports","title":"Introduction"},{"location":"#installation","text":"\u26a0\ufe0f Warning : This project is in an alpha state and is rapidly changing. Please test carefully for production usage and report any issues. $ pip install zillion","title":"Installation"},{"location":"#primer","text":"The following is meant to give a quick overview of some theory and nomenclature used in data warehousing with Zillion . Skip below for a quickstart example . In short: Zillion writes SQL for you and makes data accessible through a very simple API: result = warehouse . execute( metrics = [ \"revenue\" , \"leads\" ], dimensions = [ \"date\" ], criteria = [ ( \"date\" , \">\" , \"2020-01-01\" ), ( \"partner\" , \"=\" , \"Partner A\" ) ] )","title":"Primer"},{"location":"#metrics-and-dimensions","text":"In Zillion there are two main types of Fields that will be used in your report requests: Dimensions : attributes of data used for labelling, grouping, and filtering Metrics : facts and measures that may be broken down along dimensions A Field encapsulates the concept of a column in your data. For example, you may have a Field called \"revenue\". That Field may occur across several datasources or possibly in multiple tables within a single datasource. Zillion understands that all of those columns represent the same concept, and it can try to use any of them to satisfy reports requesting \"revenue\". Likewise there are two main types of tables used to structure your warehouse: Dimension Tables : reference/attribute tables containing only related dimensions Metric Tables : fact tables that may contain metrics and some related dimensions/attributes Dimension tables are often static or slowly growing in terms of row count and contain attributes tied to a primary key. Some common examples would be lists of US Zip Codes or company/partner directories. Metric tables are generally more transactional in nature. Some common examples would be records for web requests, ecommerce sales, or stock market price history.","title":"Metrics and Dimensions"},{"location":"#warehouse-theory","text":"If you really want to go deep on dimensional modeling and the drill-across querying technique Zillion employs, I recommend reading Ralph Kimball's book on data warehousing. To summarize, drill-across querying forms one or more queries to satisfy a report request for metrics that may exist across multiple datasources and/or tables at a particular dimension grain. Zillion technically supports snowflake or star schemas, though it isn't picky about it. You can specify table relationships through a parent-child lineage, and Zillion can also infer acceptable joins based on the presence of dimension table primary keys.","title":"Warehouse Theory"},{"location":"#query-layers","text":"Zillion reports can be thought of as running in two layers: DataSource Layer : SQL queries against the warehouse's datasources Combined Layer : A final SQL query against the combined data from the DataSource Layer The Combined Layer is just another SQL database (in-memory SQLite by default) that is used to tie the datasource data together and apply a few additional features such as rollups, row filters, row limits, sorting, pivots, and technical computations.","title":"Query Layers"},{"location":"#executing-reports","text":"The main purpose of Zillion is to execute reports against a Warehouse . You'll see how to initialize a Warehouse in a bit, but at a high level you will be crafting reports as follows: result = warehouse . execute( metrics = [ \"revenue\" , \"leads\" ], dimensions = [ \"date\" ], criteria = [ ( \"date\" , \">\" , \"2020-01-01\" ), ( \"partner\" , \"=\" , \"Partner A\" ) ] ) print(result . df) # Pandas DataFrame When comparing to writing SQL, it's helpful to think of the dimensions as the target columns of a group by SQL statement. Think of the metrics as the columns you are aggregating . Think of the criteria as the where clause . Your criteria are applied in the DataSource Layer SQL queries. The ReportResult has a Pandas DataFrame with the dimensions as the index and the metrics as the columns. A Report is said to have a grain , which defines the dimensions each metric must be able to join to in order to satisfy the Report requirements. The grain is a combination of all dimensions, including those referenced in criteria or in metric formulas. In the example above, the grain would be {date, partner} . Both \"revenue\" and \"leads\" must be able to join to those dimensions for this report to be possible. These concepts can take time to sink in and obviously vary with the specifics of your data model, but you will become more familiar with them as you start putting together reports against your data warehouses.","title":"Executing Reports"},{"location":"#example-sales-analytics","text":"Below we will walk through a simple hypothetical sales data model that demonstrates basic DataSource and Warehouse configuration and then shows some sample reports . The data is a simple SQLite database that is part of the Zillion test code. For reference, the schema is as follows: CREATE TABLE partners ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL UNIQUE , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE campaigns ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL UNIQUE , category VARCHAR NOT NULL , partner_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE leads ( id INTEGER PRIMARY KEY , name VARCHAR NOT NULL , campaign_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); CREATE TABLE sales ( id INTEGER PRIMARY KEY , item VARCHAR NOT NULL , quantity INTEGER NOT NULL , revenue DECIMAL( 10 , 2 ), lead_id INTEGER NOT NULL , created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP );","title":"Example - Sales Analytics"},{"location":"#warehouse-creation","text":"A Warehouse may be created from an existing SQLAlchemy MetaData instance, purely from a JSON/YAML configuration, or a combination of the two. The code below shows how it can be done in as little as one line of code if you have a pointer to a JSON/YAML Warehouse config. Note : Defining your config in JSON or YAML is recommended, so we'll save an example of defining Zillion metadata directly on your SQLAlchemy objects for another time. from zillion import Warehouse wh = Warehouse(config = \"https://raw.githubusercontent.com/totalhack/zillion/master/examples/example_wh_config.json\" ) This example config uses a data_url in its DataSource connect info that tells Zillion to dynamically download that data and connect to it as a SQLite database. This is useful for quick examples or analysis, though in most scenarios you would put a connection string to an existing database like you see here The basics of Zillion's configuration structure are as follows: A Warehouse config has the following main sections: metrics : optional list of metric configs for global metrics dimensions : optional list of dimension configs for global dimensions datasources : mapping of datasource names to datasource configs or config URLs A DataSource config has the following main sections: connect : database connection url or dict of connect params metrics : optional list of metric configs specific to this datasource dimensions : optional list of dimension configs specific to this datasource tables : mapping of table names to table configs or config URLs Tip: datasource and table configs may also be replaced with a URL that points to a local or remote config file. In this example all four tables in our database are included in the config, two as dimension tables and two as metric tables. The tables are linked through a parent->child relationship: partners to campaigns, and leads to sales. Some tables also utilize the create_fields flag to automatically create Fields on the datasource from column definitions. Other metrics and dimensions are defined explicitly. To view the structure of this Warehouse after init you can use the print_info method which shows all metrics, dimensions, tables, and columns that are part of your data warehouse: wh . print_info() # Formatted print of the Warehouse structure For a deeper dive of the config schema please see the full docs .","title":"Warehouse Creation"},{"location":"#reports","text":"Example: Get sales, leads, and revenue by partner: result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" ] ) print(result . df) \"\"\" sales leads revenue partner_name Partner A 11 4 165.0 Partner B 2 2 19.0 Partner C 5 1 118.5 \"\"\" Example: Let's limit to Partner A and break down by its campaigns: result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"campaign_name\" ], criteria = [( \"partner_name\" , \"=\" , \"Partner A\" )] ) print(result . df) \"\"\" sales leads revenue campaign_name Campaign 1A 5 2 83 Campaign 2A 6 2 82 \"\"\" Example: The output below shows rollups at the campaign level within each partner, and also a rollup of totals at the partner and campaign level. Note: the output contains a special character to mark DataFrame rollup rows that were added to the result. The ReportResult object contains some helper attributes to automatically access or filter rollups, as well as a df_display attribute that returns the result with friendlier display values substituted for special characters. The under-the-hood special character is left here for illustration, but may not render the same in all scenarios. from zillion import RollupTypes result = wh . execute( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" , \"campaign_name\" ], rollup = RollupTypes . ALL ) print(result . df) \"\"\" sales leads revenue partner_name campaign_name Partner A Campaign 1A 5.0 2.0 83.0 Campaign 2A 6.0 2.0 82.0 \udbff\udfff 11.0 4.0 165.0 Partner B Campaign 1B 1.0 1.0 6.0 Campaign 2B 1.0 1.0 13.0 \udbff\udfff 2.0 2.0 19.0 Partner C Campaign 1C 5.0 1.0 118.5 \udbff\udfff 5.0 1.0 118.5 \udbff\udfff \udbff\udfff 18.0 7.0 302.5 \"\"\" See the Report docs for more information on supported rollup behavior. Example: Save a report spec (not the data): First you must make sure you have saved your Warehouse , as saved reports are scoped to a particular Warehouse ID. To save a Warehouse you must provide a URL that points to the complete config. name = \"My Unique Warehouse Name\" config_url = < some url pointing to a complete warehouse config > wh . save(name, config_url) # wh.id is populated after this spec_id = wh . save_report( metrics = [ \"sales\" , \"leads\" , \"revenue\" ], dimensions = [ \"partner_name\" ] ) Note : If you built your Warehouse in python from a list of DataSources , or passed in a dict for the config param on init, there currently is not a built-in way to output a complete config to a file for reference when saving. Example: Load and run a report from a spec ID: result = wh . execute_id(spec_id) Note: The ZILLION_CONFIG environment var can point to a yaml config file. The database used to store Zillion report specs can be configured by setting the DB_URL value in your Zillion config to a valid database connection string. By default a SQLite DB in /tmp is used. See this sample config . Environment vars prefixed with ZILLION_ can override config settings (i.e. ZILLION_DB_URL will override DB_URL). Example: Unsupported Grain If you attempt an impossible report, you will get an UnsupportedGrainException . The report below is impossible because it attempts to break down the leads metric by a dimension that only exists in a child table. Generally speaking, child tables can join back up to parents (and \"siblings\" of parents) to find dimensions, but not the other way around. # Fails with UnsupportedGrainException result = wh . execute( metrics = [ \"leads\" ], dimensions = [ \"sale_id\" ] )","title":"Reports"},{"location":"#advanced-topics","text":"","title":"Advanced Topics"},{"location":"#formula-metrics","text":"In our example above our config included a formula-based metric called \"rpl\", which is simply revenue / leads . A FormulaMetric combines other metrics and/or dimensions to calculate a new metric at the Combined Layer of querying. The syntax must match your Combined Layer database, which is SQLite in our example. { \"name\" : \"rpl\" , \"aggregation\" : \"mean\" , \"rounding\" : 2 , \"formula\" : \"{revenue}/{leads}\" }","title":"Formula Metrics"},{"location":"#datasource-formulas","text":"Our example also includes a metric \"sales\" whose value is calculated via formula at the DataSource Layer of querying. Note the following in the fields list for the \"id\" param in the \"main.sales\" table. These formulas are in the syntax of the particular DataSource database technology, which also happens to be SQLite in our example. \"fields\" : [ \"sale_id\" , { \"name\" : \"sales\" , \"ds_formula\" : \"COUNT(DISTINCT sales.id)\" } ]","title":"DataSource Formulas"},{"location":"#type-conversions","text":"Our example also automatically created a handful of dimensions from the \"created_at\" columns of the leads and sales tables. Support for automatic type conversions is limited, but for date/datetime columns in supported DataSource technologies you can get a variety of dimensions for free this way. The output of wh.print_info will show the added dimensions, which are prefixed with \"lead_\" or \"sale_\" as specified by the optional type_conversion_prefix in the config for each table. Some examples of auto-generated dimensions in our example warehouse include sale_hour, sale_day_name, sale_month, sale_year, etc. As an optimization in the where clause of underlying report queries, Zillion will try to apply conversions to criteria values instead of columns. For example, it is generally more efficient to query as my_datetime > '2020-01-01' and my_datetime < '2020-01-02' instead of DATE(my_datetime) == '2020-01-01' , because the latter can prevent index usage in many database technologies. The ability to apply conversions to values instead of columns varies by field and DataSource technology as well. To prevent type conversions, set skip_conversion_fields to true on your DataSource config. See zillion.field.TYPE_ALLOWED_CONVERSIONS and zillion.field.DIALECT_CONVERSIONS for more details on currently supported conversions.","title":"Type Conversions"},{"location":"#config-variables","text":"If you'd like to avoid putting sensitive connection information directly in your DataSource configs you can leverage config variables. In your Zillion yaml config you can specify a DATASOURCE_CONTEXTS section as follows: DATASOURCE_CONTEXTS : my_ds_name : user : user123 pass : goodpassword host : 127.0.0.1 schema : reporting Then when your DataSource config for the datasource named \"my_ds_name\" is read, it can use this context to populate variables in your connection url: \"datasources\" : { \"my_ds_name\" : { \"connect\" : \"mysql+pymysql://{user}:{pass}@{host}/{schema}\" ... } }","title":"Config Variables"},{"location":"#datasource-priority","text":"On Warehouse init you can specify a default priority order for datasources by name. This will come into play when a report could be satisfied by multiple datasources. DataSources earlier in the list will be higher priority. This would be useful if you wanted to favor a set of faster, aggregate tables that are grouped in a DataSource . wh = Warehouse(config = config, ds_priority = [ \"aggr_ds\" , \"raw_ds\" , ... ])","title":"DataSource Priority"},{"location":"#ad-hoc-metrics","text":"You may also define metrics \"ad hoc\" with each report request. Below is an example that creates a revenue-per-lead metric on the fly. These only exist within the scope of the report, and the name can not conflict with any existing fields: result = wh . execute( metrics = [ \"leads\" , { \"formula\" : \"{revenue}/{leads}\" , \"name\" : \"my_rpl\" } ], dimensions = [ \"partner_name\" ] )","title":"Ad Hoc Metrics"},{"location":"#ad-hoc-tables","text":"Zillion also supports creation or syncing of ad hoc tables in your database during DataSource or Warehouse init. An example of a table config that does this is shown here . It uses the table config's data_url and if_exists params to control the syncing and/or creation of the \"main.dma_zip\" table from a remote CSV in a SQLite database. The same can be done in other database types too. The potential performance drawbacks to such an approach should be obvious, particularly if you are initializing your warehouse often or if the remote data file is large. It is often better to sync and create your data ahead of time so you have complete schema control, but this method can be very useful in certain scenarios. \u26a0\ufe0f Warning : be careful not to overwrite existing tables in your database!","title":"Ad Hoc Tables"},{"location":"#technicals","text":"There are a variety of technical computations that can be applied to metrics to compute rolling, cumulative, or rank statistics. For example, to compute a 5-point moving average on revenue one might define a new metric as follows: { \"name\" : \"revenue_ma_5\" , \"type\" : \"numeric(10,2)\" , \"aggregation\" : \"sum\" , \"rounding\" : 2 , \"technical\" : \"mean(5)\" } Technical computations are computed at the Combined Layer, whereas the \"aggregation\" is done at the DataSource Layer (hence needing to define both above). For more info on how shorthand technical strings are parsed, see the parse_technical_string code. For a full list of supported technical types see zillion.core.TechnicalTypes . Technicals also support two modes: \"group\" and \"all\". The mode controls how to apply the technical computation across the data's dimensions. In \"group\" mode, it computes the technical across the last dimension, whereas in \"all\" mode in computes the technical across all data without any regard for dimensions. The point of this becomes more clear if you try to do a \"cumsum\" technical across data broken down by something like [\"partner_name\", \"date\"]. If \"group\" mode is used (the default in most cases) it will do cumulative sums within each partner over the date ranges. If \"all\" mode is used, it will do a cumulative sum across every data row. You can be explicit about the mode by appending it to the technical string: i.e. \"cumsum:all\" or \"mean(5):group\"","title":"Technicals"},{"location":"#supported-datasources","text":"Zillion's goal is to support any database technology that SQLAlchemy supports. That said the support and testing levels in Zillion vary at the moment. In particular, the ability to do type conversions, database reflection, and kill running queries all require some database-specific code for support. The following list summarizes known support levels. Your mileage may vary with untested database technologies that SQLAlchemy supports (it might work just fine, just hasn't been tested yet). Please report bugs and help add more support! SQLite: supported and tested MySQL: supported and tested PostgreSQL: supported and lightly tested MSSQL: not tested, but support is planned Oracle: not tested, but support is planned BigQuery, Redshift, Snowflake, etc: not tested, but support is planned Note that this is different than the database support for the Combined Layer database. Currently only SQLite is supported there, though it is planned to make this more generic such that any SQLAlchemy supported database could be used.","title":"Supported DataSources"},{"location":"#multiprocess-considerations","text":"If you plan to run Zillion in a multiprocess scenario, whether on a single node or across multiple nodes, there are a couple of things to consider: SQLite DataSources do not scale well and may run into locking issues with multiple processes trying to access them on the same node. Any file-based database technology that isn't centrally accessible would be challenging when using multiple nodes. Ad Hoc DataSource and Ad Hoc Table downloads should be avoided as they may conflict/repeat across each process. Offload this to an external ETL process that is better suited to manage those data flows in a scalable production scenario. Note that you can still use the default SQLite in-memory Combined Layer DB without issues, as that is made on the fly with each report request and requires no coordination/communication with other processes or nodes.","title":"Multiprocess Considerations"},{"location":"#related-projects","text":"Awesome Zillion - A collection of Zillion projects and resources. Zillion Web UI - A demo UI and web API for Zillion","title":"Related Projects"},{"location":"api/","text":"API Reference \u00b6 zillion.configs zillion.core zillion.datasource zillion.dialects zillion.field zillion.model zillion.report zillion.scripts zillion.sql_utils zillion.version zillion.warehouse","title":"API Reference"},{"location":"api/#api-reference","text":"zillion.configs zillion.core zillion.datasource zillion.dialects zillion.field zillion.model zillion.report zillion.scripts zillion.sql_utils zillion.version zillion.warehouse","title":"API Reference"},{"location":"contributing/","text":"Your help and feedback are greatly appreciated. Whether it's supporting/testing a new datasource type, finding bugs, or suggesting features, every little bit helps make Zillion reach its potential. Please also consider manicuring or configuring datasets that others may find useful. With as little as a CSV and a short JSON configuration file you can give back to the community. You can host these shared datasources easily with GitHub. How to Contribute \u00b6 Check for open issues or open a new issue to start a discussion around a feature idea or a bug. Fork the repository on GitHub to start making your changes to the master branch (or branch off of it). Write a test which shows that the bug was fixed or that the feature works as expected. Send a pull request . Add yourself to AUTHORS . Development Setup \u00b6 # Clone this repo git clone https://github.com/totalhack/zillion.git cd zillion # Install dependencies # Note: activate your venv first if desired! pip install \".[dev]\" # Bring up test databases -- test data will init the first time # You can optionally run these DBs directly on your machine instead docker-compose up # Run tests export ZILLION_CONFIG = $( pwd ) /tests/test_config.yaml cd tests pytest Good Bug Reports \u00b6 Please be aware of the following things when filing bug reports: Avoid raising duplicate issues. Please use the GitHub issue search feature to check whether your bug report or feature request has been mentioned in the past. Duplicate bug reports and feature requests are a huge maintenance burden on the limited resources of the project. If it is clear from your report that you would have struggled to find the original, that's ok, but if searching for a selection of words in your issue title would have found the duplicate then the issue will likely be closed. When filing bug reports about exceptions or tracebacks, please include the complete traceback. Partial tracebacks, or just the exception text, are not helpful. Issues that do not contain complete tracebacks may be closed without warning. Make sure you provide a suitable amount of information to work with. Questions \u00b6 The GitHub issue tracker is for bug reports and feature requests . Please do not use it to ask questions about how to use Zillion.","title":"Contributing"},{"location":"contributing/#how-to-contribute","text":"Check for open issues or open a new issue to start a discussion around a feature idea or a bug. Fork the repository on GitHub to start making your changes to the master branch (or branch off of it). Write a test which shows that the bug was fixed or that the feature works as expected. Send a pull request . Add yourself to AUTHORS .","title":"How to Contribute"},{"location":"contributing/#development-setup","text":"# Clone this repo git clone https://github.com/totalhack/zillion.git cd zillion # Install dependencies # Note: activate your venv first if desired! pip install \".[dev]\" # Bring up test databases -- test data will init the first time # You can optionally run these DBs directly on your machine instead docker-compose up # Run tests export ZILLION_CONFIG = $( pwd ) /tests/test_config.yaml cd tests pytest","title":"Development Setup"},{"location":"contributing/#good-bug-reports","text":"Please be aware of the following things when filing bug reports: Avoid raising duplicate issues. Please use the GitHub issue search feature to check whether your bug report or feature request has been mentioned in the past. Duplicate bug reports and feature requests are a huge maintenance burden on the limited resources of the project. If it is clear from your report that you would have struggled to find the original, that's ok, but if searching for a selection of words in your issue title would have found the duplicate then the issue will likely be closed. When filing bug reports about exceptions or tracebacks, please include the complete traceback. Partial tracebacks, or just the exception text, are not helpful. Issues that do not contain complete tracebacks may be closed without warning. Make sure you provide a suitable amount of information to work with.","title":"Good Bug Reports"},{"location":"contributing/#questions","text":"The GitHub issue tracker is for bug reports and feature requests . Please do not use it to ask questions about how to use Zillion.","title":"Questions"},{"location":"zillion.configs/","text":"Module zillion.configs \u00b6 AdHocFieldSchema \u00b6 Bases : zillion.configs.FormulaFieldConfigSchema class zillion.configs. AdHocFieldSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) Base schema for an adhoc field AdHocMetricSchema \u00b6 Bases : zillion.configs.AdHocFieldSchema class zillion.configs. AdHocMetricSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of an adhoc metric Attributes: aggregation - ( str, optional ) A string representing the aggregation type to apply to this metric. See zillion.core.AggregationTypes . technical - ( str or dict, optional ) A string or dict that will be parsed as a TechnicalField to define a technical computation to be applied to the metric. rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. BaseSchema \u00b6 Bases : marshmallow.schema.Schema class zillion.configs. BaseSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) Base Schema with custom JSON module Attributes: meta - (*dict, optional) A dict of additional custom attributes for the config object BollingerTechnical \u00b6 Bases : zillion.configs.RollingTechnical class zillion.configs. BollingerTechnical ( type , params , mode=None ) Compute a rolling average and bollinger bands for a column. This adds additional columns to the input dataframe. apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string ColumnConfigSchema \u00b6 Bases : zillion.configs.ColumnInfoSchema class zillion.configs. ColumnConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a column configuration ColumnFieldConfigField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. ColumnFieldConfigField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A marshmallow field for the column's field attribute ColumnFieldConfigSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. ColumnFieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a column's field attribute Attributes: name - ( str ) The name of the field ds_formula - ( str ) A formula used to calculate the field value at the datasource query level. It must use syntax specific to the datasource. ColumnInfo \u00b6 Bases : zillion.configs.ZillionInfo, tlbx.logging_utils.PrintMixin class zillion.configs. ColumnInfo ( **kwargs ) ZillionInfo for a column in a table. See ColumnInfoSchema for more details about fields. add_field ( self , field ) Add the field to the column's fields create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema field_ds_formula ( self , name ) Get the datasource-level formula for a field if it exists get_criteria_conversion ( self , field_name , operation ) Get the datasource-level criteria conversion for a field/operation get_field ( self , name ) Get the reference to the field defined on this column. This may return a string or a dict depending on how the field was defined on the column. Parameters: name - ( str ) The name of the field Returns: ( str or dict ) - The name of the field or the dict defining the field get_field_names ( self ) Get the names of all fields mapped on this column get_fields ( self ) Get all fields mapped on this column has_field ( self , field ) Determine if the column supports the given field has_field_ds_formula ( self , name ) True if a datasource-level formula for a field exists schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing ColumnInfoSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. ColumnInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of column info that ends up in the zillion column metadata Attributes: fields - ( list of ColumnFieldConfigField, optional ) A list of field names or definitions allow_type_conversions - ( bool, optional ) A flag denoting whether additional fields may be inferred from this column based on its column type (such as deriving year from a date). type_conversion_prefix - ( str, optional ) A prefix to apply to all fields defined through automated type conversions. active - ( bool, optional ) A flag denoting whether this column is active. required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this column. ConfigMixin \u00b6 class zillion.configs. ConfigMixin ( *args , **kwargs ) Mixin to allow validation against a marshmallow schema from_config ( config ) Create a the object from a config to_config ( self ) Get the config for this object DataSourceConfigField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. DataSourceConfigField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a datasource configuration represented as a marshmallow Field DataSourceConfigSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. DataSourceConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a datasource configuration Attributes: connect - ( str or dict ) A connection string or dict for establishing the datasource connection. This may have placeholders that get filled in from the DATASOURCE_CONTEXTS of the zillion config. See DataSourceConnectField for more details on passing a dict. skip_conversion_fields - ( bool, optional ) Don't add any conversion fields when applying a config prefix_with - ( str, optional ) prefix all queries against this DataSource using SQLAlchemy's prefix_with function. The table-level prefix_with setting overrides this setting. metrics - ( marshmallow field, optional ) A list of MetricConfigSchema dimensions - ( marshmallow field, optional ) A list of DimensionConfigSchema tables - ( marshmallow field, optional ) A dict mapping of TableNameField -> TableConfigSchema DataSourceConnectField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. DataSourceConnectField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a datasource connect field DataSourceConnectSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. DataSourceConnectSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a technical configuration DataSourceCriteriaConversionsField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. DataSourceCriteriaConversionsField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining column-level criteria conversions. This allows for optimizing queries by converting values instead of applying a function on the column to evaluate criteria, which can otherwise prevent index usage. DiffTechnical \u00b6 Bases : zillion.configs.PandasTechnical class zillion.configs. DiffTechnical ( type , params , mode=None ) A Technical that computes a periodic diff on a DataFrame apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string DimensionConfigSchema \u00b6 Bases : zillion.configs.FieldConfigSchema class zillion.configs. DimensionConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a dimension configuration Attributes: values - ( str or list, optional ) A list of allowed dimension values or a name of a callable to provide a list of values. If a string representing a callable is passed, it must be importable and the callable must accept two arguments: (warehouse ID, dimension object). An example callable would be zillion.field.values_from_db which reads allowed dimension values from the dimension_values table in the Zillion database. sorter - ( str, optional ) A reference to an importable callable that accepts three arguments: (warehouse ID, dimension object, values). Currently values is a pandas Series and the callable is expected to return a Series. See zillion.field.sort_by_value_order for an example. DimensionValuesField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. DimensionValuesField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining dimension values FieldConfigSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. FieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The based schema of a field configuration Attributes: name - ( str ) The name of the field type - ( str ) A string representing the data type of the field. This will be converted to a SQLAlchemy type via ast.literal_eval . display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field FormulaFieldConfigSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. FormulaFieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The based schema of a formula field configuration Attributes: name - ( str ) The name of the field formula - ( str, optional ) A formula used to compute the field value. Formula fields are applied at the combined query layer, rather than in datasources queries, so the syntax must match that of the combined query layer database. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field FormulaMetricConfigSchema \u00b6 Bases : zillion.configs.FormulaFieldConfigSchema, zillion.configs.MetricConfigSchemaMixin class zillion.configs. FormulaMetricConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a formula metric configuration MetricConfigSchema \u00b6 Bases : zillion.configs.FieldConfigSchema, zillion.configs.MetricConfigSchemaMixin class zillion.configs. MetricConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a metric configuration MetricConfigSchemaMixin \u00b6 class zillion.configs. MetricConfigSchemaMixin ( ) Common attributes and logic for metric configs Attributes: aggregation - ( str, optional ) A string representing the aggregation type to apply to this metric. See zillion.core.AggregationTypes . rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( str or dict, optional ) A string or dict that will be parsed as a TechnicalField to define a technical computation to be applied to the metric. required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. PandasTechnical \u00b6 Bases : zillion.configs.Technical class zillion.configs. PandasTechnical ( type , params , mode=None ) A generic Technical runs a pandas method apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string PolyNested \u00b6 Bases : marshmallow.fields.Nested class zillion.configs. PolyNested ( nested , * , default= , only=None , exclude=() , many=False , unknown=None , **kwargs ) A polytype nested field that iterates through a list of possible types RankTechnical \u00b6 Bases : zillion.configs.PandasTechnical class zillion.configs. RankTechnical ( type , params , mode=None ) A Technical specific to the pandas rank function apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string RollingTechnical \u00b6 Bases : zillion.configs.Technical class zillion.configs. RollingTechnical ( type , params , mode=None ) A Technical that uses the pandas rolling feature apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string TableConfigSchema \u00b6 Bases : zillion.configs.TableInfoSchema class zillion.configs. TableConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a table configuration Attributes: columns - ( dict, optional ) A dict mapping of column name to ColumnConfigSchema data_url - ( str, optional ) A url used to download table data if this is an adhoc table if_exists - ( str, optional ) Control whether to replace, fail, or ignore when the table data already exists. drop_dupes - ( bool, optional ) Drop duplicate primary key rows when loading a table from a data_url convert_types - ( dict, optional ) A mapping of column names to types to convert to when loading a table from a data url. The types must be strings representing valid sqlalchemy types. Ex: {\"col1\": \"date\", \"col2\": \"integer\"} primary_key - ( list of str, optional ) A list of fields representing the primary key of the table adhoc_table_options - ( dict, optional ) A dict of additional params to pass to the adhoc table class as kwargs TableInfo \u00b6 Bases : zillion.configs.ZillionInfo, tlbx.logging_utils.PrintMixin class zillion.configs. TableInfo ( **kwargs ) ZillionInfo for a table. See TableInfoSchema for more details about fields. create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing TableInfoSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. TableInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of table info that ends up in the zillion table metadata Attributes: type - ( str ) Specifies the TableType active - ( bool, optional ) A flag denoting whether this table is active or not. parent - ( str, optional ) A reference to the full name of a parent table. This impacts the possible join relationships of this table. It is assumed to be safe to join back to any parent or ancestor table via shared keys (the child table must have the primary key of the parent table). siblings - ( list, optional ) A list of references to the full names of sibling tables. This impacts the possible join relationships of this table. It is assumed to be safe to join back to any sibling table via shared keys (the child table must have the primary key of the sibling table). create_fields - ( bool, optional ) If true, try to create Field objects from all columns in the table. Specifying the fields in a column config will override this behavior. Metric vs Dimension fields are inferred from the type. It is generally better to be explicit about your fields and field types, but this option provides convenience for special cases, particularly adhoc use cases. use_full_column_names - ( bool, optional ) If True and create_fields is True, fully qualify the created field names using the full table and column names. If false, assume it is safe to simply use the column name as the field name. primary_key - ( list of str ) A list of fields representing the primary key of the table incomplete_dimensions - ( list of str, optional ) If specified, a list of dimensions that are not safe to use for joins. priority - ( int, optional ) Set the priority of this table relative to other tables. All tables default to priority=1. When choosing the best table, lower numbers are considered higher priority. Tables at the same priority level use the length of their TableSet for the given query as the tie-breaker. See Warehouse._choose_best_table_set . prefix_with - ( str, optional ) prefix all queries against this Table using SQLAlchemy's prefix_with function. If a query contains multiple tables with prefix_with set, the first in the join takes precedence. TableNameField \u00b6 Bases : marshmallow.fields.String class zillion.configs. TableNameField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a table configuration represented as a marshmallow Field TableTypeField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. TableTypeField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for the type of a table Technical \u00b6 Bases : tlbx.object_utils.MappingMixin, tlbx.logging_utils.PrintMixin class zillion.configs. Technical ( type , params , mode=None ) A technical computation on a DataFrame column Parameters: type - ( str ) The TechnicalType params - ( dict ) Params for the technical computation mode - ( str ) The mode that controls how to apply the technical computation across the data's dimensions. See TechnicalModes for options. If None, the default mode will be set based on the technical type. Attributes: allowed_params - ( set ) Define the allowed technical parameters apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string TechnicalField \u00b6 Bases : marshmallow.fields.Field class zillion.configs. TechnicalField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining technical calculations TechnicalInfoSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. TechnicalInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a technical configuration WarehouseConfigSchema \u00b6 Bases : zillion.configs.BaseSchema class zillion.configs. WarehouseConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a warehouse configuration. Attributes: includes - ( marshmallow field, optional ) A list of warehouse files to import. Later items in the list will override earlier items for overlapping keys. Any settings in the warehouse config will take precedence. metrics - ( marshmallow field, optional ) A list of MetricConfigSchema dimensions - ( marshmallow field, optional ) A list of DimensionConfigSchema datasources - ( marshmallow field ) A dict mapping of datasource name -> DataSourceConfigField ZillionInfo \u00b6 Bases : tlbx.object_utils.MappingMixin class zillion.configs. ZillionInfo ( **kwargs ) Information that defines a part of the zillion configuration. The information may come from a JSON config file or directly from the SQLALchemy object's info.zillion attribute. The JSON schema is parsed with a marshmallow schema object. See the particular schema used with each subclass for details on fields. Parameters: kwargs - Parameters that will be parsed with the given marshmallow schema. Attributes: schema - ( marshmallow schema ) A class attribute that specifies the marshmallow schema used to parse the input args on init. create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing create_technical \u00b6 zillion.configs. create_technical ( info ) Create a technical instance from the input object Parameters: info - ( str or dict ) If str, parse as atechnical string. If a dict, parse as TechnicalInfoSchema. Returns: ( Technical ) - A Technical object based on the input. default_field_display_name \u00b6 zillion.configs. default_field_display_name ( name ) Determine a default display name from the field name Parameters: name - ( str ) The field name to process Returns: ( str ) - The field display name default_field_name \u00b6 zillion.configs. default_field_name ( column ) Get the default field name from a SQLAlchemy column Parameters: column - ( SQLAlchemy column ) A column to get the default field name for Returns: ( str ) - The default field name for the column field_safe_name \u00b6 zillion.configs. field_safe_name ( name ) Replace characters with underscores if they are not in FIELD_NAME_ALLOWED_CHARS Parameters: name - ( str ) The field name to process Returns: ( str ) - The \"safe\" field name has_valid_sqlalchemy_type_values \u00b6 zillion.configs. has_valid_sqlalchemy_type_values ( val ) Validate a mapping that has sqlalchemy type strings as values is_active \u00b6 zillion.configs. is_active ( obj ) Helper to test if an object is an active part of the zillion config is_valid_aggregation \u00b6 zillion.configs. is_valid_aggregation ( val ) Validate aggregation type is_valid_column_field_config \u00b6 zillion.configs. is_valid_column_field_config ( val ) Validate column field config is_valid_connect_type \u00b6 zillion.configs. is_valid_connect_type ( val ) Validate technical type is_valid_datasource_config \u00b6 zillion.configs. is_valid_datasource_config ( val ) Validate datasource config is_valid_datasource_connect \u00b6 zillion.configs. is_valid_datasource_connect ( val ) Validate datasource connect value is_valid_datasource_criteria_conversions \u00b6 zillion.configs. is_valid_datasource_criteria_conversions ( val ) Validate datasource criteria conversions is_valid_dimension_values \u00b6 zillion.configs. is_valid_dimension_values ( val ) Validate dimension values is_valid_field_display_name \u00b6 zillion.configs. is_valid_field_display_name ( val ) Validate field display name is_valid_field_name \u00b6 zillion.configs. is_valid_field_name ( val ) Validate field name is_valid_if_exists \u00b6 zillion.configs. is_valid_if_exists ( val ) Validate if_exists param is_valid_sqlalchemy_type \u00b6 zillion.configs. is_valid_sqlalchemy_type ( val ) Validate SQLAlchemy type string is_valid_table_name \u00b6 zillion.configs. is_valid_table_name ( val ) Validate table name is_valid_table_type \u00b6 zillion.configs. is_valid_table_type ( val ) Validate table type is_valid_technical \u00b6 zillion.configs. is_valid_technical ( val ) Validate technical is_valid_technical_mode \u00b6 zillion.configs. is_valid_technical_mode ( val ) Validate technical mode is_valid_technical_type \u00b6 zillion.configs. is_valid_technical_type ( val ) Validate technical type load_datasource_config \u00b6 zillion.configs. load_datasource_config ( cfg ) Parse a datasource JSON config Parameters: cfg - ( dict, str, or buffer ) A datasource config dict or a file path/buffer to read the config contents from. Returns: ( dict ) - The parsed datasource config load_datasource_config_from_env \u00b6 zillion.configs. load_datasource_config_from_env ( var ) Parse a datasource JSON config from a location stored in an environment variable load_warehouse_config \u00b6 zillion.configs. load_warehouse_config ( cfg ) Parse a warehouse JSON config Parameters: cfg - ( dict, str, or buffer ) A warehouse config dict or a file path/buffer to read the config contents from. Returns: ( dict ) - The parsed warehouse config load_warehouse_config_from_env \u00b6 zillion.configs. load_warehouse_config_from_env ( var ) Parse a warehouse JSON config from a location stored in an environment variable parse_schema_file \u00b6 zillion.configs. parse_schema_file ( f , schema ) Parse a marshmallow schema file Parameters: f - ( str or buffer ) A file path or buffer to read the raw schema contents from. Both JSON and YAML are supported. schema - ( marshmallow schema ) The marshmallow schema to use to parse the data Returns: ( dict ) - A dict structure loaded from the schema file parse_technical_string \u00b6 zillion.configs. parse_technical_string ( val ) Parse Technical args from a shorthand string Parameters: val - ( str ) The technical string to parse. The general format is: type(*args):mode . The type must be a valid value in TechnicalTypes. The argument requirements vary by type, and are optional in some cases. The mode controls whether the computation is done across the last group or the full data. The mode is optional, and will default to a value specific to that technical type (usually \"group\" mode). Examples: \"mean(5)\" for moving average, window=5 \"mean(5,2)\" for moving average, window=5, min_period=2 \"cumsum\" for cumulative sum (no args) \"cumsum:all\" for cumulative sum across all data, regardless of dimension Returns: ( dict ) - A dict of Technical args","title":"zillion.configs"},{"location":"zillion.configs/#module-zillionconfigs","text":"","title":"Module zillion.configs"},{"location":"zillion.configs/#adhocfieldschema","text":"Bases : zillion.configs.FormulaFieldConfigSchema class zillion.configs. AdHocFieldSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) Base schema for an adhoc field","title":"AdHocFieldSchema"},{"location":"zillion.configs/#adhocmetricschema","text":"Bases : zillion.configs.AdHocFieldSchema class zillion.configs. AdHocMetricSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of an adhoc metric Attributes: aggregation - ( str, optional ) A string representing the aggregation type to apply to this metric. See zillion.core.AggregationTypes . technical - ( str or dict, optional ) A string or dict that will be parsed as a TechnicalField to define a technical computation to be applied to the metric. rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric.","title":"AdHocMetricSchema"},{"location":"zillion.configs/#baseschema","text":"Bases : marshmallow.schema.Schema class zillion.configs. BaseSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) Base Schema with custom JSON module Attributes: meta - (*dict, optional) A dict of additional custom attributes for the config object","title":"BaseSchema"},{"location":"zillion.configs/#bollingertechnical","text":"Bases : zillion.configs.RollingTechnical class zillion.configs. BollingerTechnical ( type , params , mode=None ) Compute a rolling average and bollinger bands for a column. This adds additional columns to the input dataframe. apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"BollingerTechnical"},{"location":"zillion.configs/#columnconfigschema","text":"Bases : zillion.configs.ColumnInfoSchema class zillion.configs. ColumnConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a column configuration","title":"ColumnConfigSchema"},{"location":"zillion.configs/#columnfieldconfigfield","text":"Bases : marshmallow.fields.Field class zillion.configs. ColumnFieldConfigField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A marshmallow field for the column's field attribute","title":"ColumnFieldConfigField"},{"location":"zillion.configs/#columnfieldconfigschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. ColumnFieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a column's field attribute Attributes: name - ( str ) The name of the field ds_formula - ( str ) A formula used to calculate the field value at the datasource query level. It must use syntax specific to the datasource.","title":"ColumnFieldConfigSchema"},{"location":"zillion.configs/#columninfo","text":"Bases : zillion.configs.ZillionInfo, tlbx.logging_utils.PrintMixin class zillion.configs. ColumnInfo ( **kwargs ) ZillionInfo for a column in a table. See ColumnInfoSchema for more details about fields. add_field ( self , field ) Add the field to the column's fields create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema field_ds_formula ( self , name ) Get the datasource-level formula for a field if it exists get_criteria_conversion ( self , field_name , operation ) Get the datasource-level criteria conversion for a field/operation get_field ( self , name ) Get the reference to the field defined on this column. This may return a string or a dict depending on how the field was defined on the column. Parameters: name - ( str ) The name of the field Returns: ( str or dict ) - The name of the field or the dict defining the field get_field_names ( self ) Get the names of all fields mapped on this column get_fields ( self ) Get all fields mapped on this column has_field ( self , field ) Determine if the column supports the given field has_field_ds_formula ( self , name ) True if a datasource-level formula for a field exists schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing","title":"ColumnInfo"},{"location":"zillion.configs/#columninfoschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. ColumnInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of column info that ends up in the zillion column metadata Attributes: fields - ( list of ColumnFieldConfigField, optional ) A list of field names or definitions allow_type_conversions - ( bool, optional ) A flag denoting whether additional fields may be inferred from this column based on its column type (such as deriving year from a date). type_conversion_prefix - ( str, optional ) A prefix to apply to all fields defined through automated type conversions. active - ( bool, optional ) A flag denoting whether this column is active. required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this column.","title":"ColumnInfoSchema"},{"location":"zillion.configs/#configmixin","text":"class zillion.configs. ConfigMixin ( *args , **kwargs ) Mixin to allow validation against a marshmallow schema from_config ( config ) Create a the object from a config to_config ( self ) Get the config for this object","title":"ConfigMixin"},{"location":"zillion.configs/#datasourceconfigfield","text":"Bases : marshmallow.fields.Field class zillion.configs. DataSourceConfigField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a datasource configuration represented as a marshmallow Field","title":"DataSourceConfigField"},{"location":"zillion.configs/#datasourceconfigschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. DataSourceConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a datasource configuration Attributes: connect - ( str or dict ) A connection string or dict for establishing the datasource connection. This may have placeholders that get filled in from the DATASOURCE_CONTEXTS of the zillion config. See DataSourceConnectField for more details on passing a dict. skip_conversion_fields - ( bool, optional ) Don't add any conversion fields when applying a config prefix_with - ( str, optional ) prefix all queries against this DataSource using SQLAlchemy's prefix_with function. The table-level prefix_with setting overrides this setting. metrics - ( marshmallow field, optional ) A list of MetricConfigSchema dimensions - ( marshmallow field, optional ) A list of DimensionConfigSchema tables - ( marshmallow field, optional ) A dict mapping of TableNameField -> TableConfigSchema","title":"DataSourceConfigSchema"},{"location":"zillion.configs/#datasourceconnectfield","text":"Bases : marshmallow.fields.Field class zillion.configs. DataSourceConnectField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a datasource connect field","title":"DataSourceConnectField"},{"location":"zillion.configs/#datasourceconnectschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. DataSourceConnectSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a technical configuration","title":"DataSourceConnectSchema"},{"location":"zillion.configs/#datasourcecriteriaconversionsfield","text":"Bases : marshmallow.fields.Field class zillion.configs. DataSourceCriteriaConversionsField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining column-level criteria conversions. This allows for optimizing queries by converting values instead of applying a function on the column to evaluate criteria, which can otherwise prevent index usage.","title":"DataSourceCriteriaConversionsField"},{"location":"zillion.configs/#difftechnical","text":"Bases : zillion.configs.PandasTechnical class zillion.configs. DiffTechnical ( type , params , mode=None ) A Technical that computes a periodic diff on a DataFrame apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"DiffTechnical"},{"location":"zillion.configs/#dimensionconfigschema","text":"Bases : zillion.configs.FieldConfigSchema class zillion.configs. DimensionConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a dimension configuration Attributes: values - ( str or list, optional ) A list of allowed dimension values or a name of a callable to provide a list of values. If a string representing a callable is passed, it must be importable and the callable must accept two arguments: (warehouse ID, dimension object). An example callable would be zillion.field.values_from_db which reads allowed dimension values from the dimension_values table in the Zillion database. sorter - ( str, optional ) A reference to an importable callable that accepts three arguments: (warehouse ID, dimension object, values). Currently values is a pandas Series and the callable is expected to return a Series. See zillion.field.sort_by_value_order for an example.","title":"DimensionConfigSchema"},{"location":"zillion.configs/#dimensionvaluesfield","text":"Bases : marshmallow.fields.Field class zillion.configs. DimensionValuesField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining dimension values","title":"DimensionValuesField"},{"location":"zillion.configs/#fieldconfigschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. FieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The based schema of a field configuration Attributes: name - ( str ) The name of the field type - ( str ) A string representing the data type of the field. This will be converted to a SQLAlchemy type via ast.literal_eval . display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field","title":"FieldConfigSchema"},{"location":"zillion.configs/#formulafieldconfigschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. FormulaFieldConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The based schema of a formula field configuration Attributes: name - ( str ) The name of the field formula - ( str, optional ) A formula used to compute the field value. Formula fields are applied at the combined query layer, rather than in datasources queries, so the syntax must match that of the combined query layer database. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field","title":"FormulaFieldConfigSchema"},{"location":"zillion.configs/#formulametricconfigschema","text":"Bases : zillion.configs.FormulaFieldConfigSchema, zillion.configs.MetricConfigSchemaMixin class zillion.configs. FormulaMetricConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a formula metric configuration","title":"FormulaMetricConfigSchema"},{"location":"zillion.configs/#metricconfigschema","text":"Bases : zillion.configs.FieldConfigSchema, zillion.configs.MetricConfigSchemaMixin class zillion.configs. MetricConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a metric configuration","title":"MetricConfigSchema"},{"location":"zillion.configs/#metricconfigschemamixin","text":"class zillion.configs. MetricConfigSchemaMixin ( ) Common attributes and logic for metric configs Attributes: aggregation - ( str, optional ) A string representing the aggregation type to apply to this metric. See zillion.core.AggregationTypes . rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( str or dict, optional ) A string or dict that will be parsed as a TechnicalField to define a technical computation to be applied to the metric. required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric.","title":"MetricConfigSchemaMixin"},{"location":"zillion.configs/#pandastechnical","text":"Bases : zillion.configs.Technical class zillion.configs. PandasTechnical ( type , params , mode=None ) A generic Technical runs a pandas method apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"PandasTechnical"},{"location":"zillion.configs/#polynested","text":"Bases : marshmallow.fields.Nested class zillion.configs. PolyNested ( nested , * , default= , only=None , exclude=() , many=False , unknown=None , **kwargs ) A polytype nested field that iterates through a list of possible types","title":"PolyNested"},{"location":"zillion.configs/#ranktechnical","text":"Bases : zillion.configs.PandasTechnical class zillion.configs. RankTechnical ( type , params , mode=None ) A Technical specific to the pandas rank function apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"RankTechnical"},{"location":"zillion.configs/#rollingtechnical","text":"Bases : zillion.configs.Technical class zillion.configs. RollingTechnical ( type , params , mode=None ) A Technical that uses the pandas rolling feature apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"RollingTechnical"},{"location":"zillion.configs/#tableconfigschema","text":"Bases : zillion.configs.TableInfoSchema class zillion.configs. TableConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a table configuration Attributes: columns - ( dict, optional ) A dict mapping of column name to ColumnConfigSchema data_url - ( str, optional ) A url used to download table data if this is an adhoc table if_exists - ( str, optional ) Control whether to replace, fail, or ignore when the table data already exists. drop_dupes - ( bool, optional ) Drop duplicate primary key rows when loading a table from a data_url convert_types - ( dict, optional ) A mapping of column names to types to convert to when loading a table from a data url. The types must be strings representing valid sqlalchemy types. Ex: {\"col1\": \"date\", \"col2\": \"integer\"} primary_key - ( list of str, optional ) A list of fields representing the primary key of the table adhoc_table_options - ( dict, optional ) A dict of additional params to pass to the adhoc table class as kwargs","title":"TableConfigSchema"},{"location":"zillion.configs/#tableinfo","text":"Bases : zillion.configs.ZillionInfo, tlbx.logging_utils.PrintMixin class zillion.configs. TableInfo ( **kwargs ) ZillionInfo for a table. See TableInfoSchema for more details about fields. create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing","title":"TableInfo"},{"location":"zillion.configs/#tableinfoschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. TableInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of table info that ends up in the zillion table metadata Attributes: type - ( str ) Specifies the TableType active - ( bool, optional ) A flag denoting whether this table is active or not. parent - ( str, optional ) A reference to the full name of a parent table. This impacts the possible join relationships of this table. It is assumed to be safe to join back to any parent or ancestor table via shared keys (the child table must have the primary key of the parent table). siblings - ( list, optional ) A list of references to the full names of sibling tables. This impacts the possible join relationships of this table. It is assumed to be safe to join back to any sibling table via shared keys (the child table must have the primary key of the sibling table). create_fields - ( bool, optional ) If true, try to create Field objects from all columns in the table. Specifying the fields in a column config will override this behavior. Metric vs Dimension fields are inferred from the type. It is generally better to be explicit about your fields and field types, but this option provides convenience for special cases, particularly adhoc use cases. use_full_column_names - ( bool, optional ) If True and create_fields is True, fully qualify the created field names using the full table and column names. If false, assume it is safe to simply use the column name as the field name. primary_key - ( list of str ) A list of fields representing the primary key of the table incomplete_dimensions - ( list of str, optional ) If specified, a list of dimensions that are not safe to use for joins. priority - ( int, optional ) Set the priority of this table relative to other tables. All tables default to priority=1. When choosing the best table, lower numbers are considered higher priority. Tables at the same priority level use the length of their TableSet for the given query as the tie-breaker. See Warehouse._choose_best_table_set . prefix_with - ( str, optional ) prefix all queries against this Table using SQLAlchemy's prefix_with function. If a query contains multiple tables with prefix_with set, the first in the join takes precedence.","title":"TableInfoSchema"},{"location":"zillion.configs/#tablenamefield","text":"Bases : marshmallow.fields.String class zillion.configs. TableNameField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) The schema of a table configuration represented as a marshmallow Field","title":"TableNameField"},{"location":"zillion.configs/#tabletypefield","text":"Bases : marshmallow.fields.Field class zillion.configs. TableTypeField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for the type of a table","title":"TableTypeField"},{"location":"zillion.configs/#technical","text":"Bases : tlbx.object_utils.MappingMixin, tlbx.logging_utils.PrintMixin class zillion.configs. Technical ( type , params , mode=None ) A technical computation on a DataFrame column Parameters: type - ( str ) The TechnicalType params - ( dict ) Params for the technical computation mode - ( str ) The mode that controls how to apply the technical computation across the data's dimensions. See TechnicalModes for options. If None, the default mode will be set based on the technical type. Attributes: allowed_params - ( set ) Define the allowed technical parameters apply ( self , df , column , rounding=None ) Apply a technical computation to a dataframe. If the dataframe has a multilevel index and the technical is being applied in group mode, then the data will be sliced along the second to last level and the technical applied to each subgroup. Otherwise the technical is applied across the entire dataframe. The technical is applied to the dataframe in place. Parameters: df - ( DataFrame ) A DataFrame to apply a technical computation to column - ( str ) The name of the target column for the technical computation rounding - ( dict, optional ) The rounding settings for the report's columns get_default_mode ( ) Get the default mode for applying the technical calculation parse_technical_string_params ( val ) Return named params from a technical string","title":"Technical"},{"location":"zillion.configs/#technicalfield","text":"Bases : marshmallow.fields.Field class zillion.configs. TechnicalField ( * , default= , missing= , data_key=None , attribute=None , validate=None , required=False , allow_none=None , load_only=False , dump_only=False , error_messages=None , **metadata ) A field for defining technical calculations","title":"TechnicalField"},{"location":"zillion.configs/#technicalinfoschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. TechnicalInfoSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a technical configuration","title":"TechnicalInfoSchema"},{"location":"zillion.configs/#warehouseconfigschema","text":"Bases : zillion.configs.BaseSchema class zillion.configs. WarehouseConfigSchema ( * , only=None , exclude=() , many=False , context=None , load_only=() , dump_only=() , partial=False , unknown=None ) The schema of a warehouse configuration. Attributes: includes - ( marshmallow field, optional ) A list of warehouse files to import. Later items in the list will override earlier items for overlapping keys. Any settings in the warehouse config will take precedence. metrics - ( marshmallow field, optional ) A list of MetricConfigSchema dimensions - ( marshmallow field, optional ) A list of DimensionConfigSchema datasources - ( marshmallow field ) A dict mapping of datasource name -> DataSourceConfigField","title":"WarehouseConfigSchema"},{"location":"zillion.configs/#zillioninfo","text":"Bases : tlbx.object_utils.MappingMixin class zillion.configs. ZillionInfo ( **kwargs ) Information that defines a part of the zillion configuration. The information may come from a JSON config file or directly from the SQLALchemy object's info.zillion attribute. The JSON schema is parsed with a marshmallow schema object. See the particular schema used with each subclass for details on fields. Parameters: kwargs - Parameters that will be parsed with the given marshmallow schema. Attributes: schema - ( marshmallow schema ) A class attribute that specifies the marshmallow schema used to parse the input args on init. create ( zillion_info , unknown='raise' ) Factory to create a ZillionInfo object from the class schema schema_load ( zillion_info , unknown='raise' ) Load an info dict with a marshmallow schema Parameters: zillion_info - ( dict ) A dict to load with the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing Returns: ( dict ) - The loaded schema result schema_validate ( zillion_info , unknown='raise' ) Validate an info dict against a schema. Parameters: zillion_info - ( dict ) A dict to validate against the schema unknown - ( optional ) A flag passed through to marshmallow's schema processing","title":"ZillionInfo"},{"location":"zillion.configs/#create_technical","text":"zillion.configs. create_technical ( info ) Create a technical instance from the input object Parameters: info - ( str or dict ) If str, parse as atechnical string. If a dict, parse as TechnicalInfoSchema. Returns: ( Technical ) - A Technical object based on the input.","title":"create_technical"},{"location":"zillion.configs/#default_field_display_name","text":"zillion.configs. default_field_display_name ( name ) Determine a default display name from the field name Parameters: name - ( str ) The field name to process Returns: ( str ) - The field display name","title":"default_field_display_name"},{"location":"zillion.configs/#default_field_name","text":"zillion.configs. default_field_name ( column ) Get the default field name from a SQLAlchemy column Parameters: column - ( SQLAlchemy column ) A column to get the default field name for Returns: ( str ) - The default field name for the column","title":"default_field_name"},{"location":"zillion.configs/#field_safe_name","text":"zillion.configs. field_safe_name ( name ) Replace characters with underscores if they are not in FIELD_NAME_ALLOWED_CHARS Parameters: name - ( str ) The field name to process Returns: ( str ) - The \"safe\" field name","title":"field_safe_name"},{"location":"zillion.configs/#has_valid_sqlalchemy_type_values","text":"zillion.configs. has_valid_sqlalchemy_type_values ( val ) Validate a mapping that has sqlalchemy type strings as values","title":"has_valid_sqlalchemy_type_values"},{"location":"zillion.configs/#is_active","text":"zillion.configs. is_active ( obj ) Helper to test if an object is an active part of the zillion config","title":"is_active"},{"location":"zillion.configs/#is_valid_aggregation","text":"zillion.configs. is_valid_aggregation ( val ) Validate aggregation type","title":"is_valid_aggregation"},{"location":"zillion.configs/#is_valid_column_field_config","text":"zillion.configs. is_valid_column_field_config ( val ) Validate column field config","title":"is_valid_column_field_config"},{"location":"zillion.configs/#is_valid_connect_type","text":"zillion.configs. is_valid_connect_type ( val ) Validate technical type","title":"is_valid_connect_type"},{"location":"zillion.configs/#is_valid_datasource_config","text":"zillion.configs. is_valid_datasource_config ( val ) Validate datasource config","title":"is_valid_datasource_config"},{"location":"zillion.configs/#is_valid_datasource_connect","text":"zillion.configs. is_valid_datasource_connect ( val ) Validate datasource connect value","title":"is_valid_datasource_connect"},{"location":"zillion.configs/#is_valid_datasource_criteria_conversions","text":"zillion.configs. is_valid_datasource_criteria_conversions ( val ) Validate datasource criteria conversions","title":"is_valid_datasource_criteria_conversions"},{"location":"zillion.configs/#is_valid_dimension_values","text":"zillion.configs. is_valid_dimension_values ( val ) Validate dimension values","title":"is_valid_dimension_values"},{"location":"zillion.configs/#is_valid_field_display_name","text":"zillion.configs. is_valid_field_display_name ( val ) Validate field display name","title":"is_valid_field_display_name"},{"location":"zillion.configs/#is_valid_field_name","text":"zillion.configs. is_valid_field_name ( val ) Validate field name","title":"is_valid_field_name"},{"location":"zillion.configs/#is_valid_if_exists","text":"zillion.configs. is_valid_if_exists ( val ) Validate if_exists param","title":"is_valid_if_exists"},{"location":"zillion.configs/#is_valid_sqlalchemy_type","text":"zillion.configs. is_valid_sqlalchemy_type ( val ) Validate SQLAlchemy type string","title":"is_valid_sqlalchemy_type"},{"location":"zillion.configs/#is_valid_table_name","text":"zillion.configs. is_valid_table_name ( val ) Validate table name","title":"is_valid_table_name"},{"location":"zillion.configs/#is_valid_table_type","text":"zillion.configs. is_valid_table_type ( val ) Validate table type","title":"is_valid_table_type"},{"location":"zillion.configs/#is_valid_technical","text":"zillion.configs. is_valid_technical ( val ) Validate technical","title":"is_valid_technical"},{"location":"zillion.configs/#is_valid_technical_mode","text":"zillion.configs. is_valid_technical_mode ( val ) Validate technical mode","title":"is_valid_technical_mode"},{"location":"zillion.configs/#is_valid_technical_type","text":"zillion.configs. is_valid_technical_type ( val ) Validate technical type","title":"is_valid_technical_type"},{"location":"zillion.configs/#load_datasource_config","text":"zillion.configs. load_datasource_config ( cfg ) Parse a datasource JSON config Parameters: cfg - ( dict, str, or buffer ) A datasource config dict or a file path/buffer to read the config contents from. Returns: ( dict ) - The parsed datasource config","title":"load_datasource_config"},{"location":"zillion.configs/#load_datasource_config_from_env","text":"zillion.configs. load_datasource_config_from_env ( var ) Parse a datasource JSON config from a location stored in an environment variable","title":"load_datasource_config_from_env"},{"location":"zillion.configs/#load_warehouse_config","text":"zillion.configs. load_warehouse_config ( cfg ) Parse a warehouse JSON config Parameters: cfg - ( dict, str, or buffer ) A warehouse config dict or a file path/buffer to read the config contents from. Returns: ( dict ) - The parsed warehouse config","title":"load_warehouse_config"},{"location":"zillion.configs/#load_warehouse_config_from_env","text":"zillion.configs. load_warehouse_config_from_env ( var ) Parse a warehouse JSON config from a location stored in an environment variable","title":"load_warehouse_config_from_env"},{"location":"zillion.configs/#parse_schema_file","text":"zillion.configs. parse_schema_file ( f , schema ) Parse a marshmallow schema file Parameters: f - ( str or buffer ) A file path or buffer to read the raw schema contents from. Both JSON and YAML are supported. schema - ( marshmallow schema ) The marshmallow schema to use to parse the data Returns: ( dict ) - A dict structure loaded from the schema file","title":"parse_schema_file"},{"location":"zillion.configs/#parse_technical_string","text":"zillion.configs. parse_technical_string ( val ) Parse Technical args from a shorthand string Parameters: val - ( str ) The technical string to parse. The general format is: type(*args):mode . The type must be a valid value in TechnicalTypes. The argument requirements vary by type, and are optional in some cases. The mode controls whether the computation is done across the last group or the full data. The mode is optional, and will default to a value specific to that technical type (usually \"group\" mode). Examples: \"mean(5)\" for moving average, window=5 \"mean(5,2)\" for moving average, window=5, min_period=2 \"cumsum\" for cumulative sum (no args) \"cumsum:all\" for cumulative sum across all data, regardless of dimension Returns: ( dict ) - A dict of Technical args","title":"parse_technical_string"},{"location":"zillion.core/","text":"Module zillion.core \u00b6 AggregationTypes \u00b6 class zillion.core. AggregationTypes ( ) Allowed aggregation types. These aggregations are limited by what can be done in most SQL databases DataSourceQueryModes \u00b6 class zillion.core. DataSourceQueryModes ( ) Allowed datasource query modes ExecutionState \u00b6 class zillion.core. ExecutionState ( ) Allowed report/query execution states FieldTypes \u00b6 class zillion.core. FieldTypes ( ) Allowed field types IfExistsModes \u00b6 class zillion.core. IfExistsModes ( ) Allowed modes when creating tables from data. This is based off of pandas if_exists param in the DataFrame.to_sql method, with the addition of an \"ignore\" option. The \"append\" option is also removed for now since there isn't a safe/generic way to guarantee a proper primary key has been set on the table. IfFileExistsModes \u00b6 Bases : zillion.core.IfExistsModes class zillion.core. IfFileExistsModes ( ) An extension of the modes above specific to downloaded files. This allows the config to specify that a downloaded file should be replaced after a certain amount of time. See code that uses this for implementation details. OrderByTypes \u00b6 class zillion.core. OrderByTypes ( ) Allowed Order By Types RollupTypes \u00b6 class zillion.core. RollupTypes ( ) Allowed Rollup Types TableTypes \u00b6 class zillion.core. TableTypes ( ) Allowed table types TechnicalModes \u00b6 class zillion.core. TechnicalModes ( ) Allowed Technical modes Attributes: GROUP - ( str ) Apply the technical to the last grouping of the data for a multi-dimensional report ALL - ( str ) Apply the technical across all result data TechnicalTypes \u00b6 class zillion.core. TechnicalTypes ( ) Allowed technical types dbg \u00b6 zillion.core. dbg ( msg , **kwargs ) Call tlbx dbg with zillion logger dbgsql \u00b6 zillion.core. dbgsql ( msg , **kwargs ) Call tlbx dbgsql with zillion logger dictmerge \u00b6 zillion.core. dictmerge ( x , y , path=None , overwrite=False , extend=False ) Adapted version of tlbx's dictmerge that supports extending lists download_file \u00b6 zillion.core. download_file ( url , outfile=None ) Utility to download a datafile error \u00b6 zillion.core. error ( msg , **kwargs ) Call tlbx error with zillion logger get_modified_time \u00b6 zillion.core. get_modified_time ( fname ) Utility to get the modified time of a file get_time_since_modified \u00b6 zillion.core. get_time_since_modified ( fname ) Utility to get the time since a file was last modified get_zillion_config_log_level \u00b6 zillion.core. get_zillion_config_log_level ( ) igetattr \u00b6 zillion.core. igetattr ( obj , attr , *args ) Case-insensitive getattr info \u00b6 zillion.core. info ( msg , **kwargs ) Call tlbx info with zillion logger load_json_or_yaml_from_str \u00b6 zillion.core. load_json_or_yaml_from_str ( string , f=None , schema=None ) Load the file as json or yaml depending on the extension (if f is a string) or by trying both (if f is not a string). If you know ahead of time that your data is one or the other, you should use yaml or json load directly. Parameters: string - ( str ) The raw json or yaml string f - ( str or buffer ) A file path or buffer where contents were read from schema - ( optional ) Validate against this schema Returns: ( dict ) - A dict structure loaded from the json/yaml load_yaml \u00b6 zillion.core. load_yaml ( fname ) Wrapper to safe_load that also expands environment vars load_zillion_config \u00b6 zillion.core. load_zillion_config ( ) If the ZILLION_CONFIG environment variable is defined, read the YAML config from this file. Environment variable substitution is supported in the yaml file. Otherwise return a default config. Environment variables prefixed with \"ZILLION_\" will also be read in (with the prefix stripped) and take precedence. Returns: ( dict ) - The zillion config dict. raiseif \u00b6 zillion.core. raiseif ( cond , msg='' , exc= ) Convenience assert-like utility raiseifnot \u00b6 zillion.core. raiseifnot ( cond , msg='' , exc= ) Convenience assert-like utility read_filepath_or_buffer \u00b6 zillion.core. read_filepath_or_buffer ( f , open_flags='r' , compression=None ) Open and read files or buffers, local or remote set_log_level_from_config \u00b6 zillion.core. set_log_level_from_config ( cfg ) warn \u00b6 zillion.core. warn ( msg , **kwargs ) Call tlbx warn with zillion logger","title":"zillion.core"},{"location":"zillion.core/#module-zillioncore","text":"","title":"Module zillion.core"},{"location":"zillion.core/#aggregationtypes","text":"class zillion.core. AggregationTypes ( ) Allowed aggregation types. These aggregations are limited by what can be done in most SQL databases","title":"AggregationTypes"},{"location":"zillion.core/#datasourcequerymodes","text":"class zillion.core. DataSourceQueryModes ( ) Allowed datasource query modes","title":"DataSourceQueryModes"},{"location":"zillion.core/#executionstate","text":"class zillion.core. ExecutionState ( ) Allowed report/query execution states","title":"ExecutionState"},{"location":"zillion.core/#fieldtypes","text":"class zillion.core. FieldTypes ( ) Allowed field types","title":"FieldTypes"},{"location":"zillion.core/#ifexistsmodes","text":"class zillion.core. IfExistsModes ( ) Allowed modes when creating tables from data. This is based off of pandas if_exists param in the DataFrame.to_sql method, with the addition of an \"ignore\" option. The \"append\" option is also removed for now since there isn't a safe/generic way to guarantee a proper primary key has been set on the table.","title":"IfExistsModes"},{"location":"zillion.core/#iffileexistsmodes","text":"Bases : zillion.core.IfExistsModes class zillion.core. IfFileExistsModes ( ) An extension of the modes above specific to downloaded files. This allows the config to specify that a downloaded file should be replaced after a certain amount of time. See code that uses this for implementation details.","title":"IfFileExistsModes"},{"location":"zillion.core/#orderbytypes","text":"class zillion.core. OrderByTypes ( ) Allowed Order By Types","title":"OrderByTypes"},{"location":"zillion.core/#rolluptypes","text":"class zillion.core. RollupTypes ( ) Allowed Rollup Types","title":"RollupTypes"},{"location":"zillion.core/#tabletypes","text":"class zillion.core. TableTypes ( ) Allowed table types","title":"TableTypes"},{"location":"zillion.core/#technicalmodes","text":"class zillion.core. TechnicalModes ( ) Allowed Technical modes Attributes: GROUP - ( str ) Apply the technical to the last grouping of the data for a multi-dimensional report ALL - ( str ) Apply the technical across all result data","title":"TechnicalModes"},{"location":"zillion.core/#technicaltypes","text":"class zillion.core. TechnicalTypes ( ) Allowed technical types","title":"TechnicalTypes"},{"location":"zillion.core/#dbg","text":"zillion.core. dbg ( msg , **kwargs ) Call tlbx dbg with zillion logger","title":"dbg"},{"location":"zillion.core/#dbgsql","text":"zillion.core. dbgsql ( msg , **kwargs ) Call tlbx dbgsql with zillion logger","title":"dbgsql"},{"location":"zillion.core/#dictmerge","text":"zillion.core. dictmerge ( x , y , path=None , overwrite=False , extend=False ) Adapted version of tlbx's dictmerge that supports extending lists","title":"dictmerge"},{"location":"zillion.core/#download_file","text":"zillion.core. download_file ( url , outfile=None ) Utility to download a datafile","title":"download_file"},{"location":"zillion.core/#error","text":"zillion.core. error ( msg , **kwargs ) Call tlbx error with zillion logger","title":"error"},{"location":"zillion.core/#get_modified_time","text":"zillion.core. get_modified_time ( fname ) Utility to get the modified time of a file","title":"get_modified_time"},{"location":"zillion.core/#get_time_since_modified","text":"zillion.core. get_time_since_modified ( fname ) Utility to get the time since a file was last modified","title":"get_time_since_modified"},{"location":"zillion.core/#get_zillion_config_log_level","text":"zillion.core. get_zillion_config_log_level ( )","title":"get_zillion_config_log_level"},{"location":"zillion.core/#igetattr","text":"zillion.core. igetattr ( obj , attr , *args ) Case-insensitive getattr","title":"igetattr"},{"location":"zillion.core/#info","text":"zillion.core. info ( msg , **kwargs ) Call tlbx info with zillion logger","title":"info"},{"location":"zillion.core/#load_json_or_yaml_from_str","text":"zillion.core. load_json_or_yaml_from_str ( string , f=None , schema=None ) Load the file as json or yaml depending on the extension (if f is a string) or by trying both (if f is not a string). If you know ahead of time that your data is one or the other, you should use yaml or json load directly. Parameters: string - ( str ) The raw json or yaml string f - ( str or buffer ) A file path or buffer where contents were read from schema - ( optional ) Validate against this schema Returns: ( dict ) - A dict structure loaded from the json/yaml","title":"load_json_or_yaml_from_str"},{"location":"zillion.core/#load_yaml","text":"zillion.core. load_yaml ( fname ) Wrapper to safe_load that also expands environment vars","title":"load_yaml"},{"location":"zillion.core/#load_zillion_config","text":"zillion.core. load_zillion_config ( ) If the ZILLION_CONFIG environment variable is defined, read the YAML config from this file. Environment variable substitution is supported in the yaml file. Otherwise return a default config. Environment variables prefixed with \"ZILLION_\" will also be read in (with the prefix stripped) and take precedence. Returns: ( dict ) - The zillion config dict.","title":"load_zillion_config"},{"location":"zillion.core/#raiseif","text":"zillion.core. raiseif ( cond , msg='' , exc= ) Convenience assert-like utility","title":"raiseif"},{"location":"zillion.core/#raiseifnot","text":"zillion.core. raiseifnot ( cond , msg='' , exc= ) Convenience assert-like utility","title":"raiseifnot"},{"location":"zillion.core/#read_filepath_or_buffer","text":"zillion.core. read_filepath_or_buffer ( f , open_flags='r' , compression=None ) Open and read files or buffers, local or remote","title":"read_filepath_or_buffer"},{"location":"zillion.core/#set_log_level_from_config","text":"zillion.core. set_log_level_from_config ( cfg )","title":"set_log_level_from_config"},{"location":"zillion.core/#warn","text":"zillion.core. warn ( msg , **kwargs ) Call tlbx warn with zillion logger","title":"warn"},{"location":"zillion.datasource/","text":"Module zillion.datasource \u00b6 AdHocDataTable \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. AdHocDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) Create an adhoc table from raw data that can be added to a DataSource Parameters: name - ( str ) The name of the table data - ( iterable or DataFrame ) The data to create the adhoc table from table_type - ( str ) Specify the TableType columns - ( dict, optional ) Column configuration for the table primary_key - ( list of str, optional ) A list of fields that make up the primary key of the table parent - ( str, optional ) A reference to a parent table in the same datasource if_exists - ( str, optional ) Control behavior when datatables already exist in the database drop_dupes - ( bool, optional ) Drop duplicate primary key rows when loading the table convert_types - ( dict, optional ) A mapping of column names to types to convert to when loading the table. The types must be strings representing valid sqlalchemy types. Ex: {\"col1\": \"date\", \"col2\": \"integer\"} fillna_value - ( str or int, optional ) Fill null values in primary key columns with this value before writing to a SQL database. schema - ( str, optional ) The schema in which the table resides kwargs - Keyword arguments passed to pandas.DataFrame.from_records if a DataFrame is created from iterable data get_dataframe ( self ) Get the DataFrame representation of the data table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas CSVDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. CSVDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_csv get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas DataSource \u00b6 Bases : zillion.field.FieldManagerMixin, tlbx.logging_utils.PrintMixin class zillion.datasource. DataSource ( name , metadata=None , config=None ) A component of a warehouse that houses one or more related tables Parameters: name - ( str ) The name of the datasource metadata - ( SQLAlchemy metadata, optional ) A SQLAlchemy metadata object that may have zillion configuration information defined in the table and column info.zillion attribute config - ( dict, str, or buffer, optional ) A dict adhering to the DataSourceConfigSchema or a file location to load the config from add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager apply_config ( self , config , reflect=False ) Apply a datasource config to this datasource's metadata. This will also ensure zillion info is present on the metadata, populate global fields, and rebuild the datasource graph. Parameters: config - ( dict ) The datasource config to apply reflect - ( bool, optional ) If true, use SQLAlchemy to reflect the database. Table-level reflection will also occur if any tables are created from data URLs. directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric find_descendent_tables ( self , table ) Find graph descendents of the table find_neighbor_tables ( self , table ) Find tables that can be joined to or are parents of the given table Parameters: table - ( SQLAlchemy Table ) The table to find neighbors for Returns: ( list ) - A list of NeighborTables find_possible_table_sets ( self , ds_tables_with_field , field , grain , dimension_grain ) Find table sets that meet the grain Parameters: ds_tables_with_field - ( list of tables ) A list of datasource tables that have the target field field - ( str ) The target field we are trying to cover grain - ( iterable ) The grain the table set must support dimension_grain - The subset of the grain that are requested dimensions Returns: ( list ) - A list of TableSets from_data_url ( name , data_url , config=None , if_exists='fail' , replace_after='1 days' ) Create a DataSource from a data url Parameters: name - ( str ) The name to give the datasource data_url - ( str ) A url pointing to a SQLite database to download config - ( dict, optional ) A DataSourceConfigSchema dict config. Note that the connect param of this config will be overwritten if present. if_exists - ( str, optional ) Control behavior when the database already exists replace_after - ( str, optional ) Replace the data file after this interval if if_exists is \"replace_after\". See url_connect docs for more information. Returns: ( DataSource ) - A DataSource created from the data_url and config from_datatables ( name , datatables , config=None ) Create a DataSource from a list of datatables Parameters: name - ( str ) The name to give the datasource datatables - ( list of AdHocDataTables ) A list of AdHocDataTables to use to create the DataSource config - ( dict, optional ) A DataSourceConfigSchema dict config Returns: ( DataSource ) - A DataSource created from the datatables and config get_child_field_managers ( self ) Get a list of child FieldManagers get_columns_with_field ( self , field_name ) Get a list of column objects that support a field get_dialect_name ( self ) Get the name of the SQLAlchemy metadata dialect get_dim_tables_with_dim ( self , dim_name ) Get a list of dimension table objects with the given dimension get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metric_tables_with_metric ( self , metric_name ) Get a list of metric table objects with the given metric get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager get_params ( self ) Get a simple dict representation of the datasource params. This is currently not sufficient to completely rebuild the datasource. get_possible_joins ( self , table , grain ) This takes a given table (usually a metric table) and tries to find one or more joins to each dimension of the grain. It's possible some of these joins satisfy other parts of the grain too which leaves room for consolidation, but it's also possible to have it generate independent, non-overlapping joins to meet the grain. Parameters: table - ( SQLAlchemy Table ) Table to analyze for joins to grain grain - ( iterable ) An iterable of dimension names that the given table must join to Returns: ( dict ) - A mapping of dimension -> dimension joins get_table ( self , fullname ) Get the table object from the datasource's metadata Parameters: fullname - ( str ) The full name of the table Returns: ( Table ) - The SQLAlchemy table object from the metadata get_tables_with_field ( self , field_name , table_type=None ) Get a list of Tables that have a field Parameters: field_name - ( str ) The name of the field to check for table_type - ( str, optional ) Check only this TableType Returns: ( list ) - A list of Table objects has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager has_table ( self , table ) Check whether the table is in this datasource's metadata Parameters: table - ( SQLAlchemy Table ) A SQLAlchemy table Returns: ( bool ) - True if the table's fullname is in the metadata.tables map print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_info ( self ) Print the structure of the datasource print_metrics ( self , indent=None ) Print all metrics in this FieldManager ExcelDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. ExcelDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_excel get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas GoogleSheetsDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. GoogleSheetsDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a google sheet. Parsed as a CSVDataTable. get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas HTMLDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. HTMLDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from an html table using pandas.read_html. By default it expects a table in the same format as produced by: df.reset_index().to_html(\"table.html\", index=False) get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas JSONDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. JSONDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_json get_dataframe ( self , orient='table' ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas Join \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. Join ( join_parts , field_map ) Represents a join (potentially multi-part) that will be part of a query Parameters: join_parts - ( list of JoinParts ) A list of JoinParts that will make up a single Join field_map - ( dict ) The requested fields this join is meant to satisfy add_field ( self , field ) Denote that this field is covered in this join add_fields ( self , fields ) Add multiple fields as covered in this join add_join_part_tables ( self , join_part ) Add tables from join parts to the table list combine ( join1 , join2 ) Create a new Join object that combines the parts and fields of the given joins get_covered_fields ( self ) Generate a list of all possible fields this can cover join_fields_for_table ( self , table_name ) Get a list of join fields for a particular table in the join join_parts_for_table ( self , table_name ) Get a list of JoinParts that reference a particular table JoinPart \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. JoinPart ( datasource , table_names , join_fields ) A part of a join that defines a join between two particular tables NeighborTable \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. NeighborTable ( table , join_fields ) Represents a neighboring node in the datasource graph SQLiteDataTable \u00b6 Bases : zillion.datasource.AdHocDataTable class zillion.datasource. SQLiteDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from an existing sqlite database on the local filesystem Note: the \"data\" param to AdHocDataTable is ignored. This is simply a workaround to get an AdHocDataTable reference for an existing SQLite DB without having to recreate anything from data. get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , **kwargs ) TableSet \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. TableSet ( datasource , ds_table , join , grain , target_fields ) A set of tables in a datasource that can meet a grain and provide target fields. Parameters: datasource - ( DataSource ) The DataSource containing all tables ds_table - ( Table ) A table containing a desired metric or dimension join - ( Join ) A join to related tables that satisfies the grain and provides the target fields grain - ( list of str ) A list of dimensions that must be supported by the join target_fields - ( list of str ) A list of fields being targeted get_covered_fields ( self ) Get a list of all covered fields in this table set get_covered_metrics ( self , wh ) Get a list of metrics covered by this table set Parameters: wh - ( Warehouse ) The warehouse to use as a reference for metric fields Returns: ( list of str ) - A list of metric names covered in this TableSet connect_url_to_metadata \u00b6 zillion.datasource. connect_url_to_metadata ( url , ds_name=None ) Create a bound SQLAlchemy MetaData object from a database URL. The ds_name param is used to determine datasource config context for variable substitution. data_url_to_metadata \u00b6 zillion.datasource. data_url_to_metadata ( data_url , ds_name , if_exists='fail' , replace_after='1 days' ) Create a bound SQLAlchemy MetaData object from a data URL. The ds_name param is used to determine datasource config context for variable substitution. datatable_from_config \u00b6 zillion.datasource. datatable_from_config ( name , config , schema=None , **kwargs ) Factory to create an AdHocDataTable from a given config. The type of the AdHocDataTable created will be inferred from the config[\"url\"] param. Parameters: name - ( str ) The name of the table config - ( dict ) The configuration of the table schema - ( str, optional ) The schema in which the table resides kwargs - Passed to init of the particular AdHocDataTable class Returns: ( AdHocDataTable ) - Return the created AdHocDataTable (subclass) get_adhoc_datasource_filename \u00b6 zillion.datasource. get_adhoc_datasource_filename ( ds_name ) Get the filename where the adhoc datasource will be located get_adhoc_datasource_url \u00b6 zillion.datasource. get_adhoc_datasource_url ( ds_name ) Get a connection URL for the datasource get_ds_config_context \u00b6 zillion.datasource. get_ds_config_context ( name ) Helper to get datasource context from the zillion config join_from_path \u00b6 zillion.datasource. join_from_path ( ds , path , field_map=None ) Given a path in the datasource graph, get the corresponding Join Parameters: ds - ( DataSource ) The datasource for the join path - ( list of str ) A list of tables that form a join path field_map - ( dict, optional ) Passed through to Join init Returns: ( Join ) - A Join between all tables in the path metadata_from_connect \u00b6 zillion.datasource. metadata_from_connect ( connect , ds_name ) Create a bound SQLAlchemy MetaData object from a \"connect\" param. The connect value may be a connection string or a DataSourceConnectSchema dict. See the DataSourceConnectSchema docs for more details on that format. parse_replace_after \u00b6 zillion.datasource. parse_replace_after ( replace_after ) Parse a case-insensitive interval string of the format \"number interval\". The number may be a float, and the inverval options are: seconds, minutes, hours, days, weeks. populate_url_context \u00b6 zillion.datasource. populate_url_context ( url , ds_name ) Helper to do variable replacement in URLs reflect_metadata \u00b6 zillion.datasource. reflect_metadata ( metadata , reflect_only=None ) Reflect the metadata object from the connection. If reflect_only is passed, reflect only the tables specified in that list url_connect \u00b6 zillion.datasource. url_connect ( ds_name , connect_url=None , data_url=None , if_exists='fail' , replace_after='1 days' ) A URL-based datasource connector. This is meant to be used as the \"func\" value of a DataSourceConnectSchema. Only one of connect_url or data_url may be specified. Parameters: ds_name - ( str ) The name of the datasource to get a connection for connect_url - ( str, optional ) If a connect_url is passed, it will create a bound MetaData object from that connection string. data_url - ( str, optional ) If a data_url is passed, it will first download that data (or make sure it is already downloaded) and then create a connection to that data file, which is assumed to be a SQLite database. The name of the database file will be based on the name of the datasource passed in. if_exists - ( str, optional ) If a data_url is in use, this will control handling of existing data under the same filename. If \"fail\", an exception will be raised if the file already exists. If \"ignore\", it will skip downloading the file if it exists. If \"replace\", it will create or replace the file. If \"replace_after\" it will check the age of the file and replace it after the age interval provided in the replace_after param. replace_after - ( str, optional ) If if_exists is \"replace_after, use this value to determine the age threshold. The format is \"number interval\", where the number may be an int or float and the interval options are: seconds, minutes, hours, days, weeks. Note that this only occurs when url_connect is called, which is typically on datasource init; it does not replace itself periodically while the datasource is instantiated.","title":"zillion.datasource"},{"location":"zillion.datasource/#module-zilliondatasource","text":"","title":"Module zillion.datasource"},{"location":"zillion.datasource/#adhocdatatable","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. AdHocDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) Create an adhoc table from raw data that can be added to a DataSource Parameters: name - ( str ) The name of the table data - ( iterable or DataFrame ) The data to create the adhoc table from table_type - ( str ) Specify the TableType columns - ( dict, optional ) Column configuration for the table primary_key - ( list of str, optional ) A list of fields that make up the primary key of the table parent - ( str, optional ) A reference to a parent table in the same datasource if_exists - ( str, optional ) Control behavior when datatables already exist in the database drop_dupes - ( bool, optional ) Drop duplicate primary key rows when loading the table convert_types - ( dict, optional ) A mapping of column names to types to convert to when loading the table. The types must be strings representing valid sqlalchemy types. Ex: {\"col1\": \"date\", \"col2\": \"integer\"} fillna_value - ( str or int, optional ) Fill null values in primary key columns with this value before writing to a SQL database. schema - ( str, optional ) The schema in which the table resides kwargs - Keyword arguments passed to pandas.DataFrame.from_records if a DataFrame is created from iterable data get_dataframe ( self ) Get the DataFrame representation of the data table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"AdHocDataTable"},{"location":"zillion.datasource/#csvdatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. CSVDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_csv get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"CSVDataTable"},{"location":"zillion.datasource/#datasource","text":"Bases : zillion.field.FieldManagerMixin, tlbx.logging_utils.PrintMixin class zillion.datasource. DataSource ( name , metadata=None , config=None ) A component of a warehouse that houses one or more related tables Parameters: name - ( str ) The name of the datasource metadata - ( SQLAlchemy metadata, optional ) A SQLAlchemy metadata object that may have zillion configuration information defined in the table and column info.zillion attribute config - ( dict, str, or buffer, optional ) A dict adhering to the DataSourceConfigSchema or a file location to load the config from add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager apply_config ( self , config , reflect=False ) Apply a datasource config to this datasource's metadata. This will also ensure zillion info is present on the metadata, populate global fields, and rebuild the datasource graph. Parameters: config - ( dict ) The datasource config to apply reflect - ( bool, optional ) If true, use SQLAlchemy to reflect the database. Table-level reflection will also occur if any tables are created from data URLs. directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric find_descendent_tables ( self , table ) Find graph descendents of the table find_neighbor_tables ( self , table ) Find tables that can be joined to or are parents of the given table Parameters: table - ( SQLAlchemy Table ) The table to find neighbors for Returns: ( list ) - A list of NeighborTables find_possible_table_sets ( self , ds_tables_with_field , field , grain , dimension_grain ) Find table sets that meet the grain Parameters: ds_tables_with_field - ( list of tables ) A list of datasource tables that have the target field field - ( str ) The target field we are trying to cover grain - ( iterable ) The grain the table set must support dimension_grain - The subset of the grain that are requested dimensions Returns: ( list ) - A list of TableSets from_data_url ( name , data_url , config=None , if_exists='fail' , replace_after='1 days' ) Create a DataSource from a data url Parameters: name - ( str ) The name to give the datasource data_url - ( str ) A url pointing to a SQLite database to download config - ( dict, optional ) A DataSourceConfigSchema dict config. Note that the connect param of this config will be overwritten if present. if_exists - ( str, optional ) Control behavior when the database already exists replace_after - ( str, optional ) Replace the data file after this interval if if_exists is \"replace_after\". See url_connect docs for more information. Returns: ( DataSource ) - A DataSource created from the data_url and config from_datatables ( name , datatables , config=None ) Create a DataSource from a list of datatables Parameters: name - ( str ) The name to give the datasource datatables - ( list of AdHocDataTables ) A list of AdHocDataTables to use to create the DataSource config - ( dict, optional ) A DataSourceConfigSchema dict config Returns: ( DataSource ) - A DataSource created from the datatables and config get_child_field_managers ( self ) Get a list of child FieldManagers get_columns_with_field ( self , field_name ) Get a list of column objects that support a field get_dialect_name ( self ) Get the name of the SQLAlchemy metadata dialect get_dim_tables_with_dim ( self , dim_name ) Get a list of dimension table objects with the given dimension get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metric_tables_with_metric ( self , metric_name ) Get a list of metric table objects with the given metric get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager get_params ( self ) Get a simple dict representation of the datasource params. This is currently not sufficient to completely rebuild the datasource. get_possible_joins ( self , table , grain ) This takes a given table (usually a metric table) and tries to find one or more joins to each dimension of the grain. It's possible some of these joins satisfy other parts of the grain too which leaves room for consolidation, but it's also possible to have it generate independent, non-overlapping joins to meet the grain. Parameters: table - ( SQLAlchemy Table ) Table to analyze for joins to grain grain - ( iterable ) An iterable of dimension names that the given table must join to Returns: ( dict ) - A mapping of dimension -> dimension joins get_table ( self , fullname ) Get the table object from the datasource's metadata Parameters: fullname - ( str ) The full name of the table Returns: ( Table ) - The SQLAlchemy table object from the metadata get_tables_with_field ( self , field_name , table_type=None ) Get a list of Tables that have a field Parameters: field_name - ( str ) The name of the field to check for table_type - ( str, optional ) Check only this TableType Returns: ( list ) - A list of Table objects has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager has_table ( self , table ) Check whether the table is in this datasource's metadata Parameters: table - ( SQLAlchemy Table ) A SQLAlchemy table Returns: ( bool ) - True if the table's fullname is in the metadata.tables map print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_info ( self ) Print the structure of the datasource print_metrics ( self , indent=None ) Print all metrics in this FieldManager","title":"DataSource"},{"location":"zillion.datasource/#exceldatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. ExcelDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_excel get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"ExcelDataTable"},{"location":"zillion.datasource/#googlesheetsdatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. GoogleSheetsDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a google sheet. Parsed as a CSVDataTable. get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"GoogleSheetsDataTable"},{"location":"zillion.datasource/#htmldatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. HTMLDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from an html table using pandas.read_html. By default it expects a table in the same format as produced by: df.reset_index().to_html(\"table.html\", index=False) get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"HTMLDataTable"},{"location":"zillion.datasource/#jsondatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. JSONDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from a JSON file using pandas.read_json get_dataframe ( self , orient='table' ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , method='multi' , chunksize=1000 ) Use pandas to push the adhoc table data to a SQL database. Parameters: engine - ( SQLAlchemy connection engine ) The engine used to connect to the database method - ( str, optional ) Passed through to pandas chunksize - ( int, optional ) Passed through to pandas","title":"JSONDataTable"},{"location":"zillion.datasource/#join","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. Join ( join_parts , field_map ) Represents a join (potentially multi-part) that will be part of a query Parameters: join_parts - ( list of JoinParts ) A list of JoinParts that will make up a single Join field_map - ( dict ) The requested fields this join is meant to satisfy add_field ( self , field ) Denote that this field is covered in this join add_fields ( self , fields ) Add multiple fields as covered in this join add_join_part_tables ( self , join_part ) Add tables from join parts to the table list combine ( join1 , join2 ) Create a new Join object that combines the parts and fields of the given joins get_covered_fields ( self ) Generate a list of all possible fields this can cover join_fields_for_table ( self , table_name ) Get a list of join fields for a particular table in the join join_parts_for_table ( self , table_name ) Get a list of JoinParts that reference a particular table","title":"Join"},{"location":"zillion.datasource/#joinpart","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. JoinPart ( datasource , table_names , join_fields ) A part of a join that defines a join between two particular tables","title":"JoinPart"},{"location":"zillion.datasource/#neighbortable","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. NeighborTable ( table , join_fields ) Represents a neighboring node in the datasource graph","title":"NeighborTable"},{"location":"zillion.datasource/#sqlitedatatable","text":"Bases : zillion.datasource.AdHocDataTable class zillion.datasource. SQLiteDataTable ( name , data , table_type , columns=None , primary_key=None , parent=None , if_exists='fail' , drop_dupes=False , convert_types=None , fillna_value='' , schema=None , **kwargs ) AdHocDataTable from an existing sqlite database on the local filesystem Note: the \"data\" param to AdHocDataTable is ignored. This is simply a workaround to get an AdHocDataTable reference for an existing SQLite DB without having to recreate anything from data. get_dataframe ( self ) table_exists ( self , engine ) Determine if this table exists to_sql ( self , engine , **kwargs )","title":"SQLiteDataTable"},{"location":"zillion.datasource/#tableset","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.datasource. TableSet ( datasource , ds_table , join , grain , target_fields ) A set of tables in a datasource that can meet a grain and provide target fields. Parameters: datasource - ( DataSource ) The DataSource containing all tables ds_table - ( Table ) A table containing a desired metric or dimension join - ( Join ) A join to related tables that satisfies the grain and provides the target fields grain - ( list of str ) A list of dimensions that must be supported by the join target_fields - ( list of str ) A list of fields being targeted get_covered_fields ( self ) Get a list of all covered fields in this table set get_covered_metrics ( self , wh ) Get a list of metrics covered by this table set Parameters: wh - ( Warehouse ) The warehouse to use as a reference for metric fields Returns: ( list of str ) - A list of metric names covered in this TableSet","title":"TableSet"},{"location":"zillion.datasource/#connect_url_to_metadata","text":"zillion.datasource. connect_url_to_metadata ( url , ds_name=None ) Create a bound SQLAlchemy MetaData object from a database URL. The ds_name param is used to determine datasource config context for variable substitution.","title":"connect_url_to_metadata"},{"location":"zillion.datasource/#data_url_to_metadata","text":"zillion.datasource. data_url_to_metadata ( data_url , ds_name , if_exists='fail' , replace_after='1 days' ) Create a bound SQLAlchemy MetaData object from a data URL. The ds_name param is used to determine datasource config context for variable substitution.","title":"data_url_to_metadata"},{"location":"zillion.datasource/#datatable_from_config","text":"zillion.datasource. datatable_from_config ( name , config , schema=None , **kwargs ) Factory to create an AdHocDataTable from a given config. The type of the AdHocDataTable created will be inferred from the config[\"url\"] param. Parameters: name - ( str ) The name of the table config - ( dict ) The configuration of the table schema - ( str, optional ) The schema in which the table resides kwargs - Passed to init of the particular AdHocDataTable class Returns: ( AdHocDataTable ) - Return the created AdHocDataTable (subclass)","title":"datatable_from_config"},{"location":"zillion.datasource/#get_adhoc_datasource_filename","text":"zillion.datasource. get_adhoc_datasource_filename ( ds_name ) Get the filename where the adhoc datasource will be located","title":"get_adhoc_datasource_filename"},{"location":"zillion.datasource/#get_adhoc_datasource_url","text":"zillion.datasource. get_adhoc_datasource_url ( ds_name ) Get a connection URL for the datasource","title":"get_adhoc_datasource_url"},{"location":"zillion.datasource/#get_ds_config_context","text":"zillion.datasource. get_ds_config_context ( name ) Helper to get datasource context from the zillion config","title":"get_ds_config_context"},{"location":"zillion.datasource/#join_from_path","text":"zillion.datasource. join_from_path ( ds , path , field_map=None ) Given a path in the datasource graph, get the corresponding Join Parameters: ds - ( DataSource ) The datasource for the join path - ( list of str ) A list of tables that form a join path field_map - ( dict, optional ) Passed through to Join init Returns: ( Join ) - A Join between all tables in the path","title":"join_from_path"},{"location":"zillion.datasource/#metadata_from_connect","text":"zillion.datasource. metadata_from_connect ( connect , ds_name ) Create a bound SQLAlchemy MetaData object from a \"connect\" param. The connect value may be a connection string or a DataSourceConnectSchema dict. See the DataSourceConnectSchema docs for more details on that format.","title":"metadata_from_connect"},{"location":"zillion.datasource/#parse_replace_after","text":"zillion.datasource. parse_replace_after ( replace_after ) Parse a case-insensitive interval string of the format \"number interval\". The number may be a float, and the inverval options are: seconds, minutes, hours, days, weeks.","title":"parse_replace_after"},{"location":"zillion.datasource/#populate_url_context","text":"zillion.datasource. populate_url_context ( url , ds_name ) Helper to do variable replacement in URLs","title":"populate_url_context"},{"location":"zillion.datasource/#reflect_metadata","text":"zillion.datasource. reflect_metadata ( metadata , reflect_only=None ) Reflect the metadata object from the connection. If reflect_only is passed, reflect only the tables specified in that list","title":"reflect_metadata"},{"location":"zillion.datasource/#url_connect","text":"zillion.datasource. url_connect ( ds_name , connect_url=None , data_url=None , if_exists='fail' , replace_after='1 days' ) A URL-based datasource connector. This is meant to be used as the \"func\" value of a DataSourceConnectSchema. Only one of connect_url or data_url may be specified. Parameters: ds_name - ( str ) The name of the datasource to get a connection for connect_url - ( str, optional ) If a connect_url is passed, it will create a bound MetaData object from that connection string. data_url - ( str, optional ) If a data_url is passed, it will first download that data (or make sure it is already downloaded) and then create a connection to that data file, which is assumed to be a SQLite database. The name of the database file will be based on the name of the datasource passed in. if_exists - ( str, optional ) If a data_url is in use, this will control handling of existing data under the same filename. If \"fail\", an exception will be raised if the file already exists. If \"ignore\", it will skip downloading the file if it exists. If \"replace\", it will create or replace the file. If \"replace_after\" it will check the age of the file and replace it after the age interval provided in the replace_after param. replace_after - ( str, optional ) If if_exists is \"replace_after, use this value to determine the age threshold. The format is \"number interval\", where the number may be an int or float and the interval options are: seconds, minutes, hours, days, weeks. Note that this only occurs when url_connect is called, which is typically on datasource init; it does not replace itself periodically while the datasource is instantiated.","title":"url_connect"},{"location":"zillion.dialects/","text":"Module zillion.dialects \u00b6","title":"Zillion.dialects"},{"location":"zillion.dialects/#module-zilliondialects","text":"","title":"Module zillion.dialects"},{"location":"zillion.field/","text":"Module zillion.field \u00b6 AdHocDimension \u00b6 Bases : zillion.field.AdHocField class zillion.field. AdHocDimension ( name , formula , **kwargs ) An AdHoc representation of a Dimension copy ( self ) Copy this field create ( obj ) Copy this AdHocField from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object AdHocField \u00b6 Bases : zillion.field.FormulaField class zillion.field. AdHocField ( name , formula , **kwargs ) An AdHoc representation of a field copy ( self ) Copy this field create ( obj ) Copy this AdHocField from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object AdHocMetric \u00b6 Bases : zillion.field.FormulaMetric class zillion.field. AdHocMetric ( name , formula , display_name=None , description=None , meta=None , aggregation='sum' , technical=None , rounding=None , weighting_metric=None , required_grain=None ) An AdHoc representation of a Metric Parameters: name - ( str ) The name of the metric formula - ( str ) The formula used to calculate the metric display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. copy ( self ) Copy this field create ( obj ) Create an AdHocMetric from an AdHocMetricSchema dict from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object Dimension \u00b6 Bases : zillion.field.Field class zillion.field. Dimension ( name , type , display_name=None , description=None , values=None , sorter=None , meta=None , **kwargs ) Fields that represent attributes of data that are used for grouping or filtering Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field values - ( str or list, optional ) A list of allowed dimension values or a name of a callable to provide a list of values sorter - ( str, optional ) A reference to an importable callable that accepts three arguments: (warehouse ID, dimension object, values). Currently values is a pandas Series and the callable is expected to return a Series. See zillion.field.sort_by_value_order for an example. meta - ( dict, optional ) A dict of additional custom attributes kwargs - Additional attributes stored on the field object copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True , ignore_formula=False ) Get the datasource-level sql expression for this field Parameters: column - ( Column ) A SQLAlchemy column that supports this field label - ( bool, optional ) If true, label the expression with the field name ignore_formula - ( bool, optional ) If true, don't apply any available datasource formulas get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). get_values ( self , warehouse_id , refresh=False ) Get allowed values for this Dimension Parameters: warehouse_id - ( int ) A zillion warehouse ID refresh - ( bool, optional ) Refresh the values if applicable Returns: ( list or None ) - A list of valid values or None if no value restrictions have been set. is_valid_value ( self , warehouse_id , value , ignore_none=True ) Check if a value is allowed for this Dimension Parameters: warehouse_id - ( int ) A zillion warehouse ID value - ( any ) Check if this value is valid ignore_none - ( bool ) If True, consider value=None to always be valid. Returns: ( bool ) - True if the dimension value is valid sort ( self , warehouse_id , values ) Sort the given dimension values according to the sorter Parameters: warehouse_id - ( int ) A zillion warehouse ID values - ( Series ) A pandas Series of values to sort Returns: ( Series ) - A pandas Series representing the sort order to_config ( self ) Get the config for this object Field \u00b6 Bases : zillion.configs.ConfigMixin, tlbx.logging_utils.PrintMixin class zillion.field. Field ( name , type , display_name=None , description=None , meta=None , **kwargs ) Represents the concept a column is capturing, which may be shared by columns in other tables or datasources. For example, you may have a several columns in your databases/tables that represent the concept of \"revenue\". In other words, a column is like an instance of a Field. Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes kwargs - Additional attributes stored on the field object Attributes: name - ( str ) The name of the field type - ( str ) A string representing the generic SQLAlchemy type display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes sa_type - ( SQLAlchemy type ) If a dialect-specific type object is passed in on init it will be coerced to a generic type. field_type - ( str ) A valid FieldType string schema - ( Marshmallow schema ) A FieldConfigSchema class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True , ignore_formula=False ) Get the datasource-level sql expression for this field Parameters: column - ( Column ) A SQLAlchemy column that supports this field label - ( bool, optional ) If true, label the expression with the field name ignore_formula - ( bool, optional ) If true, don't apply any available datasource formulas get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object FieldManagerMixin \u00b6 class zillion.field. FieldManagerMixin ( ) An interface for managing fields (metrics and dimensions) stored on an object. Attributes: metrics_attr - ( str ) The name of the attribute where metrics are stored dimensions_attr - ( str ) The name of the attribute where dimensions are stored add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric get_child_field_managers ( self ) Get a list of child FieldManagers get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_metrics ( self , indent=None ) Print all metrics in this FieldManager FormulaDimension \u00b6 Bases : zillion.field.FormulaField class zillion.field. FormulaDimension ( name , formula , **kwargs ) A dimension defined by a formula Parameters: name - ( str ) The name of the dimension formula - ( str ) The formula used to calculate the dimension kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object FormulaField \u00b6 Bases : zillion.field.Field class zillion.field. FormulaField ( name , formula , **kwargs ) A field defined by a formula Parameters: name - ( str ) The name of the field formula - ( str ) The formula used to calculate the field kwargs - kwargs passed to the super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object FormulaMetric \u00b6 Bases : zillion.field.FormulaField class zillion.field. FormulaMetric ( name , formula , display_name=None , description=None , meta=None , aggregation='sum' , rounding=None , weighting_metric=None , technical=None , required_grain=None , **kwargs ) A metric defined by a formula Parameters: name - ( str ) The name of the metric formula - ( str ) The formula used to calculate the metric display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object Metric \u00b6 Bases : zillion.field.Field class zillion.field. Metric ( name , type , display_name=None , description=None , meta=None , aggregation='sum' , rounding=None , weighting_metric=None , technical=None , required_grain=None , **kwargs ) Fields that represent values to be measured and possibly broken down along Dimensions Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True ) Get the datasource-level sql expression for this metric Parameters: column - ( Column ) A SQLAlchemy column that supports this metric label - ( bool, optional ) If true, label the expression with the field name get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object create_dimension \u00b6 zillion.field. create_dimension ( dim_def ) Create a Dimension object from a dict of params Parameters: dim_def - ( dict ) A dict of params to init a Dimension create_metric \u00b6 zillion.field. create_metric ( metric_def ) Create a Metric object from a dict of params Parameters: metric_def - ( dict ) A dict of params to init a metric. If a formula param is present a FormulaMetric will be created. get_conversions_for_type \u00b6 zillion.field. get_conversions_for_type ( coltype ) Get all conversions for a particular column type Parameters: coltype - A SQLAlchemy column type class Returns: ( dict ) - The conversion map for the given column type. Returns None if no conversions are found. get_dialect_type_conversions \u00b6 zillion.field. get_dialect_type_conversions ( dialect , column ) Get all conversions supported by this column type for this dialect Parameters: dialect - ( str ) SQLAlchemy dialect name column - ( Column ) SQLAlchemy column object Returns: ( list ) - A list of dicts containing datasource formulas and criteria conversions for each field this column can be converted to get_table_dimensions \u00b6 zillion.field. get_table_dimensions ( fm , table , adhoc_fms=None ) Get a list of dimensions supported by a table Parameters: fm - ( FieldManager ) An object supporting the FieldManager interface table - ( SQLAlchemy Table ) The table to get a list of supported dimensions for adhoc_fms - ( list, optional ) AdHoc FieldManagers relevant to this request Returns: ( set ) - A set of dimension names get_table_field_column \u00b6 zillion.field. get_table_field_column ( table , field_name ) Return the column within a table that supports a given field Parameters: table - ( Table ) SQLAlchemy table onject field_name - ( str ) The name of a field supported by the table Returns: ( Column ) - A SQLAlchemy column object get_table_fields \u00b6 zillion.field. get_table_fields ( table ) Get a list of field names supported by a table Parameters: table - ( SQLAlchemy Table ) The table to get a list of supported fields for Returns: ( set ) - A set of field names get_table_metrics \u00b6 zillion.field. get_table_metrics ( fm , table , adhoc_fms=None ) Get a list of metrics supported by a table Parameters: fm - ( FieldManager ) An object supporting the FieldManager interface table - ( SQLAlchemy Table ) The table to get a list of supported dimensions for adhoc_fms - ( list, optional ) AdHoc FieldManagers relevant to this request Returns: ( set ) - A set of metric names replace_non_named_formula_args \u00b6 zillion.field. replace_non_named_formula_args ( formula , column ) Do formula arg replacement but raise an error if any named args are present sort_by_value_order \u00b6 zillion.field. sort_by_value_order ( warehouse_id , field , values ) Sort values by the order of the value list defined on the field Parameters: warehouse_id - ( int ) A zillion warehouse ID field - ( Field ) A zillion Field object values - ( Series ) A pandas Series to sort Returns: ( Series ) - A pandas Series representing the sort order. If no value list is found for the field, the input values are returned as is. table_field_allows_grain \u00b6 zillion.field. table_field_allows_grain ( table , field , grain ) Check whether a field in a table is restricted by required_grain Parameters: table - ( Table ) SQLAlchemy table object field - ( str ) The name of a field in the table grain - ( list of str ) A list of dimenssions that form the target grain values_from_db \u00b6 zillion.field. values_from_db ( warehouse_id , field ) Get allowed field values from the dimension_values table. If warehouse_id is None the warehouse_id is defaulted to the value of zillion.field.FIELD_VALUE_DEFAULT_WAREHOUSE_ID . This allows pulling dimension values even when a Warehouse has not been saved. Parameters: warehouse_id - ( int ) A zillion warehouse ID field - ( Field ) A zillion Dimension object Returns: ( list or None ) - A list of valid values or None if no row is found for this dimension.","title":"zillion.field"},{"location":"zillion.field/#module-zillionfield","text":"","title":"Module zillion.field"},{"location":"zillion.field/#adhocdimension","text":"Bases : zillion.field.AdHocField class zillion.field. AdHocDimension ( name , formula , **kwargs ) An AdHoc representation of a Dimension copy ( self ) Copy this field create ( obj ) Copy this AdHocField from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"AdHocDimension"},{"location":"zillion.field/#adhocfield","text":"Bases : zillion.field.FormulaField class zillion.field. AdHocField ( name , formula , **kwargs ) An AdHoc representation of a field copy ( self ) Copy this field create ( obj ) Copy this AdHocField from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"AdHocField"},{"location":"zillion.field/#adhocmetric","text":"Bases : zillion.field.FormulaMetric class zillion.field. AdHocMetric ( name , formula , display_name=None , description=None , meta=None , aggregation='sum' , technical=None , rounding=None , weighting_metric=None , required_grain=None ) An AdHoc representation of a Metric Parameters: name - ( str ) The name of the metric formula - ( str ) The formula used to calculate the metric display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. copy ( self ) Copy this field create ( obj ) Create an AdHocMetric from an AdHocMetricSchema dict from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"AdHocMetric"},{"location":"zillion.field/#dimension","text":"Bases : zillion.field.Field class zillion.field. Dimension ( name , type , display_name=None , description=None , values=None , sorter=None , meta=None , **kwargs ) Fields that represent attributes of data that are used for grouping or filtering Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field values - ( str or list, optional ) A list of allowed dimension values or a name of a callable to provide a list of values sorter - ( str, optional ) A reference to an importable callable that accepts three arguments: (warehouse ID, dimension object, values). Currently values is a pandas Series and the callable is expected to return a Series. See zillion.field.sort_by_value_order for an example. meta - ( dict, optional ) A dict of additional custom attributes kwargs - Additional attributes stored on the field object copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True , ignore_formula=False ) Get the datasource-level sql expression for this field Parameters: column - ( Column ) A SQLAlchemy column that supports this field label - ( bool, optional ) If true, label the expression with the field name ignore_formula - ( bool, optional ) If true, don't apply any available datasource formulas get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). get_values ( self , warehouse_id , refresh=False ) Get allowed values for this Dimension Parameters: warehouse_id - ( int ) A zillion warehouse ID refresh - ( bool, optional ) Refresh the values if applicable Returns: ( list or None ) - A list of valid values or None if no value restrictions have been set. is_valid_value ( self , warehouse_id , value , ignore_none=True ) Check if a value is allowed for this Dimension Parameters: warehouse_id - ( int ) A zillion warehouse ID value - ( any ) Check if this value is valid ignore_none - ( bool ) If True, consider value=None to always be valid. Returns: ( bool ) - True if the dimension value is valid sort ( self , warehouse_id , values ) Sort the given dimension values according to the sorter Parameters: warehouse_id - ( int ) A zillion warehouse ID values - ( Series ) A pandas Series of values to sort Returns: ( Series ) - A pandas Series representing the sort order to_config ( self ) Get the config for this object","title":"Dimension"},{"location":"zillion.field/#field","text":"Bases : zillion.configs.ConfigMixin, tlbx.logging_utils.PrintMixin class zillion.field. Field ( name , type , display_name=None , description=None , meta=None , **kwargs ) Represents the concept a column is capturing, which may be shared by columns in other tables or datasources. For example, you may have a several columns in your databases/tables that represent the concept of \"revenue\". In other words, a column is like an instance of a Field. Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field. display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes kwargs - Additional attributes stored on the field object Attributes: name - ( str ) The name of the field type - ( str ) A string representing the generic SQLAlchemy type display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes sa_type - ( SQLAlchemy type ) If a dialect-specific type object is passed in on init it will be coerced to a generic type. field_type - ( str ) A valid FieldType string schema - ( Marshmallow schema ) A FieldConfigSchema class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True , ignore_formula=False ) Get the datasource-level sql expression for this field Parameters: column - ( Column ) A SQLAlchemy column that supports this field label - ( bool, optional ) If true, label the expression with the field name ignore_formula - ( bool, optional ) If true, don't apply any available datasource formulas get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"Field"},{"location":"zillion.field/#fieldmanagermixin","text":"class zillion.field. FieldManagerMixin ( ) An interface for managing fields (metrics and dimensions) stored on an object. Attributes: metrics_attr - ( str ) The name of the attribute where metrics are stored dimensions_attr - ( str ) The name of the attribute where dimensions are stored add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric get_child_field_managers ( self ) Get a list of child FieldManagers get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_metrics ( self , indent=None ) Print all metrics in this FieldManager","title":"FieldManagerMixin"},{"location":"zillion.field/#formuladimension","text":"Bases : zillion.field.FormulaField class zillion.field. FormulaDimension ( name , formula , **kwargs ) A dimension defined by a formula Parameters: name - ( str ) The name of the dimension formula - ( str ) The formula used to calculate the dimension kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"FormulaDimension"},{"location":"zillion.field/#formulafield","text":"Bases : zillion.field.Field class zillion.field. FormulaField ( name , formula , **kwargs ) A field defined by a formula Parameters: name - ( str ) The name of the field formula - ( str ) The formula used to calculate the field kwargs - kwargs passed to the super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"FormulaField"},{"location":"zillion.field/#formulametric","text":"Bases : zillion.field.FormulaField class zillion.field. FormulaMetric ( name , formula , display_name=None , description=None , meta=None , aggregation='sum' , rounding=None , weighting_metric=None , technical=None , required_grain=None , **kwargs ) A metric defined by a formula Parameters: name - ( str ) The name of the metric formula - ( str ) The formula used to calculate the metric display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , *args , **kwargs ) Raise an error if called on FormulaFields get_final_select_clause ( self , warehouse , adhoc_fms=None ) Get a SQL select clause for this formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( SQLAlchemy clause ) - A compiled sqlalchemy clause for the formula get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"FormulaMetric"},{"location":"zillion.field/#metric","text":"Bases : zillion.field.Field class zillion.field. Metric ( name , type , display_name=None , description=None , meta=None , aggregation='sum' , rounding=None , weighting_metric=None , technical=None , required_grain=None , **kwargs ) Fields that represent values to be measured and possibly broken down along Dimensions Parameters: name - ( str ) The name of the field type - ( str or SQLAlchemy type ) The column type for the field display_name - ( str, optional ) The display name of the field description - ( str, optional ) The description of the field meta - ( dict, optional ) A dict of additional custom attributes aggregation - ( str, optional ) The AggregationType to apply to the metric rounding - ( int, optional ) If specified, the number of decimal places to round to weighting_metric - ( str, optional ) A reference to a metric to use for weighting when aggregating averages technical - ( object, optional ) A Technical object or definition used to defined a technical computation to be applied to the metric required_grain - ( list of str, optional ) If specified, a list of dimensions that must be present in the dimension grain of any report that aims to include this metric. kwargs - kwargs passed to super class copy ( self ) Copy this field from_config ( config ) Create a the object from a config get_all_raw_fields ( self , warehouse , adhoc_fms=None ) Get all raw fields involved in calculating this field. Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set ) - The set of all raw fields that make up this field. get_ds_expression ( self , column , label=True ) Get the datasource-level sql expression for this metric Parameters: column - ( Column ) A SQLAlchemy column that supports this metric label - ( bool, optional ) If true, label the expression with the field name get_final_select_clause ( self , *args , **kwargs ) The sql clause used when selecting at the combined query layer get_formula_fields ( self , warehouse , depth=0 , adhoc_fms=None ) Get the fields that are part of this field's formula Parameters: warehouse - ( Warehouse ) A zillion warehouse that will contain all relevant fields depth - ( int, optional ) Track the depth of recursion into the formula adhoc_fms - ( list, optional ) A list of FieldManagers Returns: ( set, str ) - The set of all base fields involved in the formula calculation, as well as an expanded version of the formula. All fields in the expanded formula should be raw fields (i.e. not formula fields). to_config ( self ) Get the config for this object","title":"Metric"},{"location":"zillion.field/#create_dimension","text":"zillion.field. create_dimension ( dim_def ) Create a Dimension object from a dict of params Parameters: dim_def - ( dict ) A dict of params to init a Dimension","title":"create_dimension"},{"location":"zillion.field/#create_metric","text":"zillion.field. create_metric ( metric_def ) Create a Metric object from a dict of params Parameters: metric_def - ( dict ) A dict of params to init a metric. If a formula param is present a FormulaMetric will be created.","title":"create_metric"},{"location":"zillion.field/#get_conversions_for_type","text":"zillion.field. get_conversions_for_type ( coltype ) Get all conversions for a particular column type Parameters: coltype - A SQLAlchemy column type class Returns: ( dict ) - The conversion map for the given column type. Returns None if no conversions are found.","title":"get_conversions_for_type"},{"location":"zillion.field/#get_dialect_type_conversions","text":"zillion.field. get_dialect_type_conversions ( dialect , column ) Get all conversions supported by this column type for this dialect Parameters: dialect - ( str ) SQLAlchemy dialect name column - ( Column ) SQLAlchemy column object Returns: ( list ) - A list of dicts containing datasource formulas and criteria conversions for each field this column can be converted to","title":"get_dialect_type_conversions"},{"location":"zillion.field/#get_table_dimensions","text":"zillion.field. get_table_dimensions ( fm , table , adhoc_fms=None ) Get a list of dimensions supported by a table Parameters: fm - ( FieldManager ) An object supporting the FieldManager interface table - ( SQLAlchemy Table ) The table to get a list of supported dimensions for adhoc_fms - ( list, optional ) AdHoc FieldManagers relevant to this request Returns: ( set ) - A set of dimension names","title":"get_table_dimensions"},{"location":"zillion.field/#get_table_field_column","text":"zillion.field. get_table_field_column ( table , field_name ) Return the column within a table that supports a given field Parameters: table - ( Table ) SQLAlchemy table onject field_name - ( str ) The name of a field supported by the table Returns: ( Column ) - A SQLAlchemy column object","title":"get_table_field_column"},{"location":"zillion.field/#get_table_fields","text":"zillion.field. get_table_fields ( table ) Get a list of field names supported by a table Parameters: table - ( SQLAlchemy Table ) The table to get a list of supported fields for Returns: ( set ) - A set of field names","title":"get_table_fields"},{"location":"zillion.field/#get_table_metrics","text":"zillion.field. get_table_metrics ( fm , table , adhoc_fms=None ) Get a list of metrics supported by a table Parameters: fm - ( FieldManager ) An object supporting the FieldManager interface table - ( SQLAlchemy Table ) The table to get a list of supported dimensions for adhoc_fms - ( list, optional ) AdHoc FieldManagers relevant to this request Returns: ( set ) - A set of metric names","title":"get_table_metrics"},{"location":"zillion.field/#replace_non_named_formula_args","text":"zillion.field. replace_non_named_formula_args ( formula , column ) Do formula arg replacement but raise an error if any named args are present","title":"replace_non_named_formula_args"},{"location":"zillion.field/#sort_by_value_order","text":"zillion.field. sort_by_value_order ( warehouse_id , field , values ) Sort values by the order of the value list defined on the field Parameters: warehouse_id - ( int ) A zillion warehouse ID field - ( Field ) A zillion Field object values - ( Series ) A pandas Series to sort Returns: ( Series ) - A pandas Series representing the sort order. If no value list is found for the field, the input values are returned as is.","title":"sort_by_value_order"},{"location":"zillion.field/#table_field_allows_grain","text":"zillion.field. table_field_allows_grain ( table , field , grain ) Check whether a field in a table is restricted by required_grain Parameters: table - ( Table ) SQLAlchemy table object field - ( str ) The name of a field in the table grain - ( list of str ) A list of dimenssions that form the target grain","title":"table_field_allows_grain"},{"location":"zillion.field/#values_from_db","text":"zillion.field. values_from_db ( warehouse_id , field ) Get allowed field values from the dimension_values table. If warehouse_id is None the warehouse_id is defaulted to the value of zillion.field.FIELD_VALUE_DEFAULT_WAREHOUSE_ID . This allows pulling dimension values even when a Warehouse has not been saved. Parameters: warehouse_id - ( int ) A zillion warehouse ID field - ( Field ) A zillion Dimension object Returns: ( list or None ) - A list of valid values or None if no row is found for this dimension.","title":"values_from_db"},{"location":"zillion.model/","text":"Module zillion.model \u00b6","title":"Zillion.model"},{"location":"zillion.model/#module-zillionmodel","text":"","title":"Module zillion.model"},{"location":"zillion.report/","text":"Module zillion.report \u00b6 BaseCombinedResult \u00b6 class zillion.report. BaseCombinedResult ( warehouse , ds_query_results , primary_ds_dimensions , adhoc_datasources=None ) A combination of datasource query results Parameters: warehouse - ( Warehouse ) A zillion warehouse ds_query_results - ( list ) A list of DataSourceQueryResult objects primary_ds_dimensions - ( list ) A list of dimensions that will be used to create the hash primary key of the combined result table adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this combined result add_warning ( self , msg , log=True ) clean_up ( self ) Clean up any resources that can/should be cleaned up create_table ( self ) Create the combined result table get_conn ( self ) Get a database connection to the combined result database get_cursor ( self , conn ) Get a cursor from a database connection get_final_result ( self , metrics , dimensions , row_filters , rollup , pivot , order_by , limit , limit_first ) Get the final result from the combined result table load_table ( self ) Load the combined result table DataSourceQuery \u00b6 Bases : zillion.report.ExecutionStateMixin, tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQuery ( warehouse , metrics , dimensions , criteria , table_set ) Build a query to run against a particular datasource Parameters: warehouse - ( Warehouse ) A zillion warehouse metrics - ( OrderedDict ) An OrderedDict mapping metric names to Metric objects dimensions - ( OrderedDict ) An OrderedDict mapping dimension names to Dimension objects criteria - ( list ) A list of criteria to be applied when querying. See the Report docs for more details. table_set - ( TableSet ) Build the query against this set of tables that supports the requested metrics and grain add_metric ( self , metric ) Add a metric to this query Parameters: metric - ( str ) A metric name covers_field ( self , field ) Check whether a field is covered in this query Parameters: field - ( str ) A field name Returns: ( bool ) - True if this field is covered in this query covers_metric ( self , metric ) Check whether a metric is covered in this query Parameters: metric - ( str ) A metric name Returns: ( bool ) - True if this metric is covered in this query execute ( self , timeout=None , label=None ) Execute the datasource query Parameters: timeout - ( float, optional ) A query timeout in seconds label - ( str, optional ) A label to apply to the SQL query Returns: ( DataSourceQueryResult ) - The result of the SQL query get_conn ( self ) Get a connection to this query's datasource get_datasource ( self ) Get a reference to the datasource for this query get_datasource_name ( self ) Get the name of the datasource used in this query get_dialect_name ( self ) Get the name of the datasource dialect get_tables ( self ) Get a reference to the tables used in this query kill ( self , main_thread=None ) Kill this datasource query Parameters: main_thread - ( Thread, optional ) A reference to the thread that started the query. This is used as a backup for dialects that don't have a supported way to kill a query. An exception will be asynchronously raised in this thread. It is not guaranteed to actually interrupt the query. DataSourceQueryResult \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQueryResult ( query , data , duration ) The results for a DataSourceQuery Parameters: query - ( DataSourceQuery ) The DataSourceQuery that was executed data - ( iterable ) The result rows duration - ( float ) The duration of the query execution in seconds DataSourceQuerySummary \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQuerySummary ( query , data , duration ) A summary of the execution results for a DataSourceQuery Parameters: query - ( DataSourceQuery ) The DataSourceQuery that was executed data - ( iterable ) The result rows duration - ( float ) The duration of the query execution in seconds format ( self ) Return a formatted summary of the DataSourceQuery results ExecutionStateMixin \u00b6 class zillion.report. ExecutionStateMixin ( ) A mixin to manage the state of a report or query Report \u00b6 Bases : zillion.report.ExecutionStateMixin class zillion.report. Report ( warehouse , metrics=None , dimensions=None , criteria=None , row_filters=None , rollup=None , pivot=None , order_by=None , limit=None , limit_first=False , adhoc_datasources=None , allow_partial=False ) Build a report against a warehouse. On init DataSource queries are built, but nothing is executed. Parameters: warehouse - ( Warehouse ) A zillion warehouse object to run the report against metrics - ( list, optional ) A list of metric names, or dicts in the case of AdHocMetrics. These will be the measures of your report, or the statistics you are interested in computing at the given dimension grain. dimensions - ( list, optional ) A list of dimension names to control the grain of the report. You can think of dimensions similarly to the \"group by\" in a SQL query. criteria - ( list, optional ) A list of criteria to be applied when querying. Each criteria in the list is represented by a 3-item list or tuple. See core.CRITERIA_OPERATIONS for all supported operations. Note that some operations, such as \"like\", have varying behavior by datasource dialect. Some examples: [\"field_a\", \">\", 1] [\"field_b\", \"=\", \"2020-04-01\"] [\"field_c\", \"like\", \"%example%\"] [\"field_d\", \"in\", [\"a\", \"b\", \"c\"]] row_filters - ( list, optional ) A list of criteria to apply at the final step (combined query layer) to filter which rows get returned. The format here is the same as for the criteria arg, though the operations are limited to the values of core.ROW_FILTER_OPERATIONS . rollup - ( str or int, optional ) Controls how metrics are rolled up / aggregated by dimension depth. If not passed no rollup will be computed. If the special value \"totals\" is passed, only a final tally rollup row will be added. If an int, then it controls the maximum depth to roll up the data, starting from the most granular (last) dimension of the report. Note that the rollup=3 case is like adding a totals row to the \"=2\" case, as a totals row is a rollup of all dimension levels. Setting rollup=len(dims) is equivalent to rollup=\"all\". For example, if you ran a report with dimensions [\"a\", \"b\", \"c\"]: rollup=\"totals\" - adds a single, final rollup row rollup=\"all\" - rolls up all dimension levels rollup=1 - rolls up the first dimension only rollup=2 - rolls up the first two dimensions rollup=3 - rolls up all three dimensions Any other non-None value would raise an error pivot - ( list, optional ) A list of dimensions to pivot to columns order_by - ( list, optional ) A list of (field, asc/desc) tuples that control the ordering of the returned result limit - ( int, optional ) A limit on the number of rows returned limit_first - ( bool, optional ) Whether to apply limits before rollups/ordering adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this report allow_partial - ( boolean, optional ) Allow reports where only some metrics can meet the requested grain. Notes: The order_by and limit functionality is only applied on the final/combined result, NOT in your DataSource queries. In most cases when you are dealing with DataSource tables that are of a decent size you will want to make sure to include criteria that limit the scope of your query and/or take advantage of underlying table indexing. If you were to use order_by or limit without any criteria or dimensions, you would effectively select all rows from the underlying datasource table into memory (or at least try to). delete ( warehouse , spec_id ) Delete a saved report spec Parameters: spec_id - ( int ) The ID of a ReportSpec to delete execute ( self ) Execute the datasource queries, combine the results, and do the final result selection. Save the ReportResult on the result attribute from_params ( warehouse , params , adhoc_datasources=None ) Build a report from a set of report params Parameters: warehouse - ( Warehouse ) A zillion warehouse object params - ( dict ) A dict of Report params adhoc_datasources - ( list, optional ) A list of FieldManagers get_dimension_grain ( self ) Get the portion of the grain specific to request dimensions get_grain ( self ) Get the grain of this report, which accounts for dimension fields required in the requested dimensions, criteria, and formula-based fields. get_json ( self ) Get a JSON representation of the Report params get_params ( self ) Get a dict of params used to create the Report kill ( self , soft=False , raise_if_failed=False ) Kill a running report Parameters: soft - ( bool, optional ) If true, set the report state to killed without attempting to kill any running datasource queries. raise_if_failed - ( bool, optional ) If true, raise FailedKillException if any exceptions occurred when trying to kill datasource queries. Otherwise a warning will be emitted. load ( warehouse , spec_id , adhoc_datasources=None ) Load a report from a spec ID Parameters: warehouse - ( Warehouse ) A zillion warehouse object spec_id - ( int ) A ReportSpec ID adhoc_datasources - ( list, optional ) A list of FieldManagers load_warehouse_id_for_report ( spec_id ) Get the Warehouse ID for a particular report spec Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( dict ) - A Warehouse ID save ( self , meta=None ) Save the report spec and return the saved spec ID Parameters: meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the report Returns: ( int ) - The ID of the saved ReportSpec ReportResult \u00b6 Bases : tlbx.logging_utils.PrintMixin class zillion.report. ReportResult ( df , duration , query_summaries , metrics , dimensions , rollup , unsupported_grain_metrics=None , warnings=None ) Encapsulates a report result as well as some additional helpers and summary statistics. Parameters: df - ( DataFrame ) The DataFrame containing the final report result duration - ( float ) The report execution duration in seconds query_summaries - ( list of DataSourceQuerySummary ) Summaries of the underyling query results. metrics - ( OrderedDict ) A mapping of requested metrics to Metric objects dimensions - ( OrderedDict ) A mapping of requested dimensions to Dimension objects rollup - ( str or int ) See the Report docs for more details. unsupported_grain_metrics - ( dict ) A dictionary mapping metric names to reasons why they could not meet the requested grain of the Report. SQLiteMemoryCombinedResult \u00b6 Bases : zillion.report.BaseCombinedResult class zillion.report. SQLiteMemoryCombinedResult ( warehouse , ds_query_results , primary_ds_dimensions , adhoc_datasources=None ) Combine query results in an in-memory SQLite database add_warning ( self , msg , log=True ) clean_up ( self ) Clean up the SQLite combined result table create_table ( self ) Create a table in the SQLite database to store the combined result get_conn ( self ) Get a SQLite memory database connection get_cursor ( self , conn ) Get a SQLite cursor from the connection get_final_result ( self , metrics , dimensions , row_filters , rollup , pivot , order_by , limit , limit_first ) Get the final reseult from the combined result table Parameters: metrics - ( OrderedDict ) An OrderedDict mapping metric names to Metric objects dimensions - ( OrderedDict ) An OrderedDict mapping dimension names to Dimension objects row_filters - ( list ) A list of criteria to filter which rows get returned rollup - ( str or int ) Controls how metrics are rolled up / aggregated by dimension. See the Report docs for more details. pivot - ( list ) A list of dimensions to pivot to columns order_by - ( list ) A list of (field, asc/desc) tuples that control the ordering of the returned result limit - ( int ) A limit on the number of rows returned limit_first - ( bool, optional ) Whether to apply limits before rollups/ordering Returns: ( DataFrame ) - A DataFrame with the final report result Notes: The default ordering of operations is meant to roughly parallel that of MySQL's rollup, having, order by and limit behavior. The operations are applied in the following order: technicals, rollups, rounding, order_by, row_filters, limit, pivot. If you set limit_first=True the the row_filter and limit operations are moved ahead of the rollups: technicals, row_filters, limit, rollups, rounding, order_by, pivot. load_table ( self ) Load the combined result table","title":"zillion.report"},{"location":"zillion.report/#module-zillionreport","text":"","title":"Module zillion.report"},{"location":"zillion.report/#basecombinedresult","text":"class zillion.report. BaseCombinedResult ( warehouse , ds_query_results , primary_ds_dimensions , adhoc_datasources=None ) A combination of datasource query results Parameters: warehouse - ( Warehouse ) A zillion warehouse ds_query_results - ( list ) A list of DataSourceQueryResult objects primary_ds_dimensions - ( list ) A list of dimensions that will be used to create the hash primary key of the combined result table adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this combined result add_warning ( self , msg , log=True ) clean_up ( self ) Clean up any resources that can/should be cleaned up create_table ( self ) Create the combined result table get_conn ( self ) Get a database connection to the combined result database get_cursor ( self , conn ) Get a cursor from a database connection get_final_result ( self , metrics , dimensions , row_filters , rollup , pivot , order_by , limit , limit_first ) Get the final result from the combined result table load_table ( self ) Load the combined result table","title":"BaseCombinedResult"},{"location":"zillion.report/#datasourcequery","text":"Bases : zillion.report.ExecutionStateMixin, tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQuery ( warehouse , metrics , dimensions , criteria , table_set ) Build a query to run against a particular datasource Parameters: warehouse - ( Warehouse ) A zillion warehouse metrics - ( OrderedDict ) An OrderedDict mapping metric names to Metric objects dimensions - ( OrderedDict ) An OrderedDict mapping dimension names to Dimension objects criteria - ( list ) A list of criteria to be applied when querying. See the Report docs for more details. table_set - ( TableSet ) Build the query against this set of tables that supports the requested metrics and grain add_metric ( self , metric ) Add a metric to this query Parameters: metric - ( str ) A metric name covers_field ( self , field ) Check whether a field is covered in this query Parameters: field - ( str ) A field name Returns: ( bool ) - True if this field is covered in this query covers_metric ( self , metric ) Check whether a metric is covered in this query Parameters: metric - ( str ) A metric name Returns: ( bool ) - True if this metric is covered in this query execute ( self , timeout=None , label=None ) Execute the datasource query Parameters: timeout - ( float, optional ) A query timeout in seconds label - ( str, optional ) A label to apply to the SQL query Returns: ( DataSourceQueryResult ) - The result of the SQL query get_conn ( self ) Get a connection to this query's datasource get_datasource ( self ) Get a reference to the datasource for this query get_datasource_name ( self ) Get the name of the datasource used in this query get_dialect_name ( self ) Get the name of the datasource dialect get_tables ( self ) Get a reference to the tables used in this query kill ( self , main_thread=None ) Kill this datasource query Parameters: main_thread - ( Thread, optional ) A reference to the thread that started the query. This is used as a backup for dialects that don't have a supported way to kill a query. An exception will be asynchronously raised in this thread. It is not guaranteed to actually interrupt the query.","title":"DataSourceQuery"},{"location":"zillion.report/#datasourcequeryresult","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQueryResult ( query , data , duration ) The results for a DataSourceQuery Parameters: query - ( DataSourceQuery ) The DataSourceQuery that was executed data - ( iterable ) The result rows duration - ( float ) The duration of the query execution in seconds","title":"DataSourceQueryResult"},{"location":"zillion.report/#datasourcequerysummary","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.report. DataSourceQuerySummary ( query , data , duration ) A summary of the execution results for a DataSourceQuery Parameters: query - ( DataSourceQuery ) The DataSourceQuery that was executed data - ( iterable ) The result rows duration - ( float ) The duration of the query execution in seconds format ( self ) Return a formatted summary of the DataSourceQuery results","title":"DataSourceQuerySummary"},{"location":"zillion.report/#executionstatemixin","text":"class zillion.report. ExecutionStateMixin ( ) A mixin to manage the state of a report or query","title":"ExecutionStateMixin"},{"location":"zillion.report/#report","text":"Bases : zillion.report.ExecutionStateMixin class zillion.report. Report ( warehouse , metrics=None , dimensions=None , criteria=None , row_filters=None , rollup=None , pivot=None , order_by=None , limit=None , limit_first=False , adhoc_datasources=None , allow_partial=False ) Build a report against a warehouse. On init DataSource queries are built, but nothing is executed. Parameters: warehouse - ( Warehouse ) A zillion warehouse object to run the report against metrics - ( list, optional ) A list of metric names, or dicts in the case of AdHocMetrics. These will be the measures of your report, or the statistics you are interested in computing at the given dimension grain. dimensions - ( list, optional ) A list of dimension names to control the grain of the report. You can think of dimensions similarly to the \"group by\" in a SQL query. criteria - ( list, optional ) A list of criteria to be applied when querying. Each criteria in the list is represented by a 3-item list or tuple. See core.CRITERIA_OPERATIONS for all supported operations. Note that some operations, such as \"like\", have varying behavior by datasource dialect. Some examples: [\"field_a\", \">\", 1] [\"field_b\", \"=\", \"2020-04-01\"] [\"field_c\", \"like\", \"%example%\"] [\"field_d\", \"in\", [\"a\", \"b\", \"c\"]] row_filters - ( list, optional ) A list of criteria to apply at the final step (combined query layer) to filter which rows get returned. The format here is the same as for the criteria arg, though the operations are limited to the values of core.ROW_FILTER_OPERATIONS . rollup - ( str or int, optional ) Controls how metrics are rolled up / aggregated by dimension depth. If not passed no rollup will be computed. If the special value \"totals\" is passed, only a final tally rollup row will be added. If an int, then it controls the maximum depth to roll up the data, starting from the most granular (last) dimension of the report. Note that the rollup=3 case is like adding a totals row to the \"=2\" case, as a totals row is a rollup of all dimension levels. Setting rollup=len(dims) is equivalent to rollup=\"all\". For example, if you ran a report with dimensions [\"a\", \"b\", \"c\"]: rollup=\"totals\" - adds a single, final rollup row rollup=\"all\" - rolls up all dimension levels rollup=1 - rolls up the first dimension only rollup=2 - rolls up the first two dimensions rollup=3 - rolls up all three dimensions Any other non-None value would raise an error pivot - ( list, optional ) A list of dimensions to pivot to columns order_by - ( list, optional ) A list of (field, asc/desc) tuples that control the ordering of the returned result limit - ( int, optional ) A limit on the number of rows returned limit_first - ( bool, optional ) Whether to apply limits before rollups/ordering adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this report allow_partial - ( boolean, optional ) Allow reports where only some metrics can meet the requested grain. Notes: The order_by and limit functionality is only applied on the final/combined result, NOT in your DataSource queries. In most cases when you are dealing with DataSource tables that are of a decent size you will want to make sure to include criteria that limit the scope of your query and/or take advantage of underlying table indexing. If you were to use order_by or limit without any criteria or dimensions, you would effectively select all rows from the underlying datasource table into memory (or at least try to). delete ( warehouse , spec_id ) Delete a saved report spec Parameters: spec_id - ( int ) The ID of a ReportSpec to delete execute ( self ) Execute the datasource queries, combine the results, and do the final result selection. Save the ReportResult on the result attribute from_params ( warehouse , params , adhoc_datasources=None ) Build a report from a set of report params Parameters: warehouse - ( Warehouse ) A zillion warehouse object params - ( dict ) A dict of Report params adhoc_datasources - ( list, optional ) A list of FieldManagers get_dimension_grain ( self ) Get the portion of the grain specific to request dimensions get_grain ( self ) Get the grain of this report, which accounts for dimension fields required in the requested dimensions, criteria, and formula-based fields. get_json ( self ) Get a JSON representation of the Report params get_params ( self ) Get a dict of params used to create the Report kill ( self , soft=False , raise_if_failed=False ) Kill a running report Parameters: soft - ( bool, optional ) If true, set the report state to killed without attempting to kill any running datasource queries. raise_if_failed - ( bool, optional ) If true, raise FailedKillException if any exceptions occurred when trying to kill datasource queries. Otherwise a warning will be emitted. load ( warehouse , spec_id , adhoc_datasources=None ) Load a report from a spec ID Parameters: warehouse - ( Warehouse ) A zillion warehouse object spec_id - ( int ) A ReportSpec ID adhoc_datasources - ( list, optional ) A list of FieldManagers load_warehouse_id_for_report ( spec_id ) Get the Warehouse ID for a particular report spec Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( dict ) - A Warehouse ID save ( self , meta=None ) Save the report spec and return the saved spec ID Parameters: meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the report Returns: ( int ) - The ID of the saved ReportSpec","title":"Report"},{"location":"zillion.report/#reportresult","text":"Bases : tlbx.logging_utils.PrintMixin class zillion.report. ReportResult ( df , duration , query_summaries , metrics , dimensions , rollup , unsupported_grain_metrics=None , warnings=None ) Encapsulates a report result as well as some additional helpers and summary statistics. Parameters: df - ( DataFrame ) The DataFrame containing the final report result duration - ( float ) The report execution duration in seconds query_summaries - ( list of DataSourceQuerySummary ) Summaries of the underyling query results. metrics - ( OrderedDict ) A mapping of requested metrics to Metric objects dimensions - ( OrderedDict ) A mapping of requested dimensions to Dimension objects rollup - ( str or int ) See the Report docs for more details. unsupported_grain_metrics - ( dict ) A dictionary mapping metric names to reasons why they could not meet the requested grain of the Report.","title":"ReportResult"},{"location":"zillion.report/#sqlitememorycombinedresult","text":"Bases : zillion.report.BaseCombinedResult class zillion.report. SQLiteMemoryCombinedResult ( warehouse , ds_query_results , primary_ds_dimensions , adhoc_datasources=None ) Combine query results in an in-memory SQLite database add_warning ( self , msg , log=True ) clean_up ( self ) Clean up the SQLite combined result table create_table ( self ) Create a table in the SQLite database to store the combined result get_conn ( self ) Get a SQLite memory database connection get_cursor ( self , conn ) Get a SQLite cursor from the connection get_final_result ( self , metrics , dimensions , row_filters , rollup , pivot , order_by , limit , limit_first ) Get the final reseult from the combined result table Parameters: metrics - ( OrderedDict ) An OrderedDict mapping metric names to Metric objects dimensions - ( OrderedDict ) An OrderedDict mapping dimension names to Dimension objects row_filters - ( list ) A list of criteria to filter which rows get returned rollup - ( str or int ) Controls how metrics are rolled up / aggregated by dimension. See the Report docs for more details. pivot - ( list ) A list of dimensions to pivot to columns order_by - ( list ) A list of (field, asc/desc) tuples that control the ordering of the returned result limit - ( int ) A limit on the number of rows returned limit_first - ( bool, optional ) Whether to apply limits before rollups/ordering Returns: ( DataFrame ) - A DataFrame with the final report result Notes: The default ordering of operations is meant to roughly parallel that of MySQL's rollup, having, order by and limit behavior. The operations are applied in the following order: technicals, rollups, rounding, order_by, row_filters, limit, pivot. If you set limit_first=True the the row_filter and limit operations are moved ahead of the rollups: technicals, row_filters, limit, rollups, rounding, order_by, pivot. load_table ( self ) Load the combined result table","title":"SQLiteMemoryCombinedResult"},{"location":"zillion.scripts/","text":"Module zillion.scripts \u00b6","title":"Zillion.scripts"},{"location":"zillion.scripts/#module-zillionscripts","text":"","title":"Module zillion.scripts"},{"location":"zillion.sql_utils/","text":"Module zillion.sql_utils \u00b6 aggregation_to_sqla_func \u00b6 zillion.sql_utils. aggregation_to_sqla_func ( aggregation ) Convert an AggregationType string to a SQLAlchemy function check_metadata_url \u00b6 zillion.sql_utils. check_metadata_url ( url , confirm_exists=False ) Check validity of the metadata URL column_fullname \u00b6 zillion.sql_utils. column_fullname ( column , prefix=None ) Get a fully qualified name for a column Parameters: column - ( SQLAlchemy column ) A SQLAlchemy column object to get the full name for prefix - ( str, optional ) If specified, a manual prefix to prepend to the output string. This will automatically be separted with a \".\". Returns: ( str ) - A fully qualified column name. The exact format will vary depending on your SQLAlchemy metadata, but an example would be: schema.table.column comment \u00b6 zillion.sql_utils. comment ( self , c ) See https://github.com/sqlalchemy/sqlalchemy/wiki/CompiledComments contains_aggregation \u00b6 zillion.sql_utils. contains_aggregation ( sql ) Determine whether a SQL query contains aggregation functions. Warning: This relies on a non-exhaustive list of SQL aggregation functions to look for. This will likely need updating. Parameters: sql - ( str or sqlparse result ) The SQL query to check for aggregation functions Returns: ( bool ) - True if the SQL string contains aggregation contains_sql_keywords \u00b6 zillion.sql_utils. contains_sql_keywords ( sql ) Determine whether a SQL query contains special SQL keywords (DML, DDL, etc.) Parameters: sql - ( str or sqlparse result ) The SQL query to check for keywords Returns: ( bool ) - True if the SQL string contains keywords filter_dialect_schemas \u00b6 zillion.sql_utils. filter_dialect_schemas ( schemas , dialect ) Filter out a set of baseline/system schemas for a dialect Parameters: schemas - ( list ) A list of schema names dialect - ( str ) The name of a SQLAlchemy dialect Returns: ( list ) - A filtered list of schema names get_postgres_pid \u00b6 zillion.sql_utils. get_postgres_pid ( conn ) Helper to get the PostgreSQL connection PID get_postgres_schemas \u00b6 zillion.sql_utils. get_postgres_schemas ( conn ) Helper to list PostgreSQL schemas get_schema_and_table_name \u00b6 zillion.sql_utils. get_schema_and_table_name ( table ) Extract the schema and table name from a full table name. If the table name is not schema-qualified, return None for the schema name get_schemas \u00b6 zillion.sql_utils. get_schemas ( engine ) Inspect the SQLAlchemy engine to get a list of schemas get_sqla_criterion_expr \u00b6 zillion.sql_utils. get_sqla_criterion_expr ( column , criterion , negate=False ) Create a SQLAlchemy criterion expression Parameters: column - ( SQLAlchemy column ) A SQLAlchemy column object to be used in the expression criterion - ( 3-item iterable ) A 3-item tuple or list of the format [field, operation, value(s)]. See core.CRITERIA_OPERATIONS for supported operations. The value item may take on different formats depending on the operation. In most cases passing an iterable will result in multiple criteria of that operation being formed. For example, (\"my_field\", \"=\", [1,2,3]) would logically or 3 conditions of equality to the 3 values in the list. The \"between\" operations expect each value to be a 2-item iterable representing the lower and upper bound of the criterion. negate - ( bool, optional ) Negate the expression Returns: ( SQLAlchemy expression ) - A SQLALchemy expression representing the criterion Notes: Postgresql \"like\" is case sensitive, but mysql \"like\" is not. Postgresql also supports \"ilike\" to specify case insensitive, so one option is to look at the dialect to determine the function, but that is not supported yet. infer_aggregation_and_rounding \u00b6 zillion.sql_utils. infer_aggregation_and_rounding ( column ) Infer the aggregation and rounding settings based on the column type Parameters: column - ( SQLAlchemy column ) The column to analyze Returns: ( AggregationType, int ) - A 2-item tuple of the aggregation type and rounding to use is_numeric_type \u00b6 zillion.sql_utils. is_numeric_type ( type ) Determine if this is a numeric SQLAlchemy type is_probably_metric \u00b6 zillion.sql_utils. is_probably_metric ( column , formula=None ) Determine if a column is probably a metric. This is used when trying to automatically init/reflect a datasource and determine the field types for columns. The logic is very coarse, and should not be relied on for more than quick/convenient use cases. Parameters: column - ( SQLAlchemy column ) The column to analyze formula - ( str, optional ) A formula to calculate the column Returns: ( bool ) - True if the column is probably a metric printexpr \u00b6 zillion.sql_utils. printexpr ( expr ) Print a SQLAlchemy expression sqla_compile \u00b6 zillion.sql_utils. sqla_compile ( expr ) Compile a SQL expression Parameters: expr - ( SQLAlchemy expression ) The SQLAlchemy expression to compile Returns: ( str ) - The compiled expression string to_generic_sa_type \u00b6 zillion.sql_utils. to_generic_sa_type ( type ) Return a generic SQLAlchemy type object from a type that may be dialect- specific. This will attempt to preserve common type settings such as specified field length, scale, and precision. On error it will fall back to trying to init the generic type with no params. to_mysql_type \u00b6 zillion.sql_utils. to_mysql_type ( type ) Compile into a MySQL SQLAlchemy type to_postgresql_type \u00b6 zillion.sql_utils. to_postgresql_type ( type ) Compile into a PostgreSQL SQLAlchemy type to_sqlite_type \u00b6 zillion.sql_utils. to_sqlite_type ( type ) Compile into a SQLite SQLAlchemy type type_string_to_sa_type \u00b6 zillion.sql_utils. type_string_to_sa_type ( type_string ) Convert a field type string to a SQLAlchemy type. The type string will be evaluated as a python statement or class name to init from the SQLAlchemy top level module. Dialect-specific SQLAlchemy types are not currently supported. Parameters: type_string - ( str ) A string representing a SQLAlchemy type, such as \"Integer\", or \"String(32)\". This does a case-insensitive search and will return the first matching SQLAlchemy type. Returns: ( SQLAlchemy type object ) - An init'd SQLAlchemy type object","title":"zillion.sql_utils"},{"location":"zillion.sql_utils/#module-zillionsql_utils","text":"","title":"Module zillion.sql_utils"},{"location":"zillion.sql_utils/#aggregation_to_sqla_func","text":"zillion.sql_utils. aggregation_to_sqla_func ( aggregation ) Convert an AggregationType string to a SQLAlchemy function","title":"aggregation_to_sqla_func"},{"location":"zillion.sql_utils/#check_metadata_url","text":"zillion.sql_utils. check_metadata_url ( url , confirm_exists=False ) Check validity of the metadata URL","title":"check_metadata_url"},{"location":"zillion.sql_utils/#column_fullname","text":"zillion.sql_utils. column_fullname ( column , prefix=None ) Get a fully qualified name for a column Parameters: column - ( SQLAlchemy column ) A SQLAlchemy column object to get the full name for prefix - ( str, optional ) If specified, a manual prefix to prepend to the output string. This will automatically be separted with a \".\". Returns: ( str ) - A fully qualified column name. The exact format will vary depending on your SQLAlchemy metadata, but an example would be: schema.table.column","title":"column_fullname"},{"location":"zillion.sql_utils/#comment","text":"zillion.sql_utils. comment ( self , c ) See https://github.com/sqlalchemy/sqlalchemy/wiki/CompiledComments","title":"comment"},{"location":"zillion.sql_utils/#contains_aggregation","text":"zillion.sql_utils. contains_aggregation ( sql ) Determine whether a SQL query contains aggregation functions. Warning: This relies on a non-exhaustive list of SQL aggregation functions to look for. This will likely need updating. Parameters: sql - ( str or sqlparse result ) The SQL query to check for aggregation functions Returns: ( bool ) - True if the SQL string contains aggregation","title":"contains_aggregation"},{"location":"zillion.sql_utils/#contains_sql_keywords","text":"zillion.sql_utils. contains_sql_keywords ( sql ) Determine whether a SQL query contains special SQL keywords (DML, DDL, etc.) Parameters: sql - ( str or sqlparse result ) The SQL query to check for keywords Returns: ( bool ) - True if the SQL string contains keywords","title":"contains_sql_keywords"},{"location":"zillion.sql_utils/#filter_dialect_schemas","text":"zillion.sql_utils. filter_dialect_schemas ( schemas , dialect ) Filter out a set of baseline/system schemas for a dialect Parameters: schemas - ( list ) A list of schema names dialect - ( str ) The name of a SQLAlchemy dialect Returns: ( list ) - A filtered list of schema names","title":"filter_dialect_schemas"},{"location":"zillion.sql_utils/#get_postgres_pid","text":"zillion.sql_utils. get_postgres_pid ( conn ) Helper to get the PostgreSQL connection PID","title":"get_postgres_pid"},{"location":"zillion.sql_utils/#get_postgres_schemas","text":"zillion.sql_utils. get_postgres_schemas ( conn ) Helper to list PostgreSQL schemas","title":"get_postgres_schemas"},{"location":"zillion.sql_utils/#get_schema_and_table_name","text":"zillion.sql_utils. get_schema_and_table_name ( table ) Extract the schema and table name from a full table name. If the table name is not schema-qualified, return None for the schema name","title":"get_schema_and_table_name"},{"location":"zillion.sql_utils/#get_schemas","text":"zillion.sql_utils. get_schemas ( engine ) Inspect the SQLAlchemy engine to get a list of schemas","title":"get_schemas"},{"location":"zillion.sql_utils/#get_sqla_criterion_expr","text":"zillion.sql_utils. get_sqla_criterion_expr ( column , criterion , negate=False ) Create a SQLAlchemy criterion expression Parameters: column - ( SQLAlchemy column ) A SQLAlchemy column object to be used in the expression criterion - ( 3-item iterable ) A 3-item tuple or list of the format [field, operation, value(s)]. See core.CRITERIA_OPERATIONS for supported operations. The value item may take on different formats depending on the operation. In most cases passing an iterable will result in multiple criteria of that operation being formed. For example, (\"my_field\", \"=\", [1,2,3]) would logically or 3 conditions of equality to the 3 values in the list. The \"between\" operations expect each value to be a 2-item iterable representing the lower and upper bound of the criterion. negate - ( bool, optional ) Negate the expression Returns: ( SQLAlchemy expression ) - A SQLALchemy expression representing the criterion Notes: Postgresql \"like\" is case sensitive, but mysql \"like\" is not. Postgresql also supports \"ilike\" to specify case insensitive, so one option is to look at the dialect to determine the function, but that is not supported yet.","title":"get_sqla_criterion_expr"},{"location":"zillion.sql_utils/#infer_aggregation_and_rounding","text":"zillion.sql_utils. infer_aggregation_and_rounding ( column ) Infer the aggregation and rounding settings based on the column type Parameters: column - ( SQLAlchemy column ) The column to analyze Returns: ( AggregationType, int ) - A 2-item tuple of the aggregation type and rounding to use","title":"infer_aggregation_and_rounding"},{"location":"zillion.sql_utils/#is_numeric_type","text":"zillion.sql_utils. is_numeric_type ( type ) Determine if this is a numeric SQLAlchemy type","title":"is_numeric_type"},{"location":"zillion.sql_utils/#is_probably_metric","text":"zillion.sql_utils. is_probably_metric ( column , formula=None ) Determine if a column is probably a metric. This is used when trying to automatically init/reflect a datasource and determine the field types for columns. The logic is very coarse, and should not be relied on for more than quick/convenient use cases. Parameters: column - ( SQLAlchemy column ) The column to analyze formula - ( str, optional ) A formula to calculate the column Returns: ( bool ) - True if the column is probably a metric","title":"is_probably_metric"},{"location":"zillion.sql_utils/#printexpr","text":"zillion.sql_utils. printexpr ( expr ) Print a SQLAlchemy expression","title":"printexpr"},{"location":"zillion.sql_utils/#sqla_compile","text":"zillion.sql_utils. sqla_compile ( expr ) Compile a SQL expression Parameters: expr - ( SQLAlchemy expression ) The SQLAlchemy expression to compile Returns: ( str ) - The compiled expression string","title":"sqla_compile"},{"location":"zillion.sql_utils/#to_generic_sa_type","text":"zillion.sql_utils. to_generic_sa_type ( type ) Return a generic SQLAlchemy type object from a type that may be dialect- specific. This will attempt to preserve common type settings such as specified field length, scale, and precision. On error it will fall back to trying to init the generic type with no params.","title":"to_generic_sa_type"},{"location":"zillion.sql_utils/#to_mysql_type","text":"zillion.sql_utils. to_mysql_type ( type ) Compile into a MySQL SQLAlchemy type","title":"to_mysql_type"},{"location":"zillion.sql_utils/#to_postgresql_type","text":"zillion.sql_utils. to_postgresql_type ( type ) Compile into a PostgreSQL SQLAlchemy type","title":"to_postgresql_type"},{"location":"zillion.sql_utils/#to_sqlite_type","text":"zillion.sql_utils. to_sqlite_type ( type ) Compile into a SQLite SQLAlchemy type","title":"to_sqlite_type"},{"location":"zillion.sql_utils/#type_string_to_sa_type","text":"zillion.sql_utils. type_string_to_sa_type ( type_string ) Convert a field type string to a SQLAlchemy type. The type string will be evaluated as a python statement or class name to init from the SQLAlchemy top level module. Dialect-specific SQLAlchemy types are not currently supported. Parameters: type_string - ( str ) A string representing a SQLAlchemy type, such as \"Integer\", or \"String(32)\". This does a case-insensitive search and will return the first matching SQLAlchemy type. Returns: ( SQLAlchemy type object ) - An init'd SQLAlchemy type object","title":"type_string_to_sa_type"},{"location":"zillion.version/","text":"Module zillion.version \u00b6","title":"Zillion.version"},{"location":"zillion.version/#module-zillionversion","text":"","title":"Module zillion.version"},{"location":"zillion.warehouse/","text":"Module zillion.warehouse \u00b6 Warehouse \u00b6 Bases : zillion.field.FieldManagerMixin class zillion.warehouse. Warehouse ( config=None , datasources=None , ds_priority=None ) A reporting warehouse that contains various datasources to run queries against and combine data in report results. The warehouse may contain global definitions for metrics and dimensions, and will also perform integrity checks of any added datasources. Note that the id, name, and meta attributes will only be populated when the Warehouse is persisted or loaded from a database. Parameters: config - ( dict, str, or buffer, optional ) A dict adhering to the WarehouseConfigSchema or a file location to load the config from datasources - ( list, optional ) A list of DataSources that will make up the warehouse ds_priority - ( list, optional ) An ordered list of datasource names establishing querying priority. This comes into play when part of a report may be satisfied by multiple datasources. Datasources earlier in this list will be higher priority. add_datasource ( self , ds , skip_integrity_checks=False ) Add a datasource to this warehouse Parameters: ds - ( DataSource ) The datasource object to add skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager apply_config ( self , config , skip_integrity_checks=False ) Apply a warehouse config Parameters: config - ( dict ) A dict adhering to the WarehouseConfigSchema skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks delete ( id ) Delete a saved warehouse. Note that this does not delete any report specs that reference this warehouse ID. Parameters: id - ( int ) The ID of a Warehouse to delete delete_report ( self , spec_id ) Delete a report by spec ID Parameters: spec_id - ( int ) The ID of a report spec to delete directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric execute ( self , metrics=None , dimensions=None , criteria=None , row_filters=None , rollup=None , pivot=None , order_by=None , limit=None , limit_first=False , adhoc_datasources=None , allow_partial=False ) Build and execute a Report Returns: ( ReportResult ) - The result of the report execute_id ( self , spec_id , adhoc_datasources=None ) Build and execute a report from a spec ID Parameters: spec_id - ( int ) The ID of a report spec adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( ReportResult ) - The result of the report get_child_field_managers ( self ) Get a list of all datasources in this warehouse get_datasource ( self , name , adhoc_datasources=None ) Get the datasource object corresponding to this datasource name Parameters: name - ( str ) The name of the datasource adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( DataSource ) - The matching datasource object get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimension_table_set ( self , grain , dimension_grain , adhoc_datasources=None ) Get a TableSet that can satisfy dimension table joins across this grain Parameters: grain - ( list ) A list of dimension names representing the full grain required including dimension and criteria grain dimension_grain - ( list of str ) A list of dimension names representing the requested dimensions for report grouping adhoc_datasources - ( list, optional ) A list of FieldManagers for this request Returns: ( TableSet ) - A TableSet that can satisfy this request get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metric_table_set ( self , metric , grain , dimension_grain , adhoc_datasources=None ) Get a TableSet that can satisfy a metric at a given grain Parameters: metric - ( str ) A metric name grain - ( list ) A list of dimension names representing the full grain required including dimension and criteria grain dimension_grain - ( list of str ) A list of dimension names representing the requested dimensions for report grouping adhoc_datasources - ( list, optional ) A list of FieldManagers for this request Returns: ( TableSet ) - A TableSet that can satisfy this request get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager load ( id ) Load a Warehouse from a Warehouse ID Parameters: id - ( int ) A Warehouse ID Returns: ( Warehouse ) - A Warehouse object load_report ( self , spec_id , adhoc_datasources=None ) Load a report from a spec ID Parameters: spec_id - ( int ) The ID of a report spec adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( Report ) - A report built from this report spec load_report_and_warehouse ( spec_id ) Load a Report and Warehouse from a ReportSpec. The Warehouse will be populated on the returned Report object. Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( Report ) - A Report built from this report spec load_warehouse_for_report ( spec_id ) Load the warehouse corresponding to the ReportSpec Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( Warehouse ) - A Warehouse object print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_info ( self ) Print the warehouse structure print_metrics ( self , indent=None ) Print all metrics in this FieldManager remove_datasource ( self , ds , skip_integrity_checks=False ) Remove a datasource from this config Parameters: ds - ( DataSource ) The datasource object to remove skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks run_integrity_checks ( self , adhoc_datasources=None ) Run a series of integrity checks on the warehouse and its datasources. This will raise a WarehouseException with all failed checks. Parameters: adhoc_datasources - ( list, optional ) A list of FieldManagers to include for this request save ( self , name , config_url , meta=None ) Save the warehouse config and return the ID Parameters: name - ( str ) A name to give the Warehouse config_url - ( str ) A URL pointing to a config file that can be used to recreate the warehouse meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the warehouse Returns: ( int ) - The ID of the saved Warehouse save_report ( self , meta=None , **kwargs ) Init a Report and save it as a ReportSpec. Note that the Warehouse must be saved before any ReportSpecs can be saved for the Warehouse. Parameters: meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the report **kwargs - Passed through to Report Returns: ( Report ) - The built report with the spec ID populated","title":"zillion.warehouse"},{"location":"zillion.warehouse/#module-zillionwarehouse","text":"","title":"Module zillion.warehouse"},{"location":"zillion.warehouse/#warehouse","text":"Bases : zillion.field.FieldManagerMixin class zillion.warehouse. Warehouse ( config=None , datasources=None , ds_priority=None ) A reporting warehouse that contains various datasources to run queries against and combine data in report results. The warehouse may contain global definitions for metrics and dimensions, and will also perform integrity checks of any added datasources. Note that the id, name, and meta attributes will only be populated when the Warehouse is persisted or loaded from a database. Parameters: config - ( dict, str, or buffer, optional ) A dict adhering to the WarehouseConfigSchema or a file location to load the config from datasources - ( list, optional ) A list of DataSources that will make up the warehouse ds_priority - ( list, optional ) An ordered list of datasource names establishing querying priority. This comes into play when part of a report may be satisfied by multiple datasources. Datasources earlier in this list will be higher priority. add_datasource ( self , ds , skip_integrity_checks=False ) Add a datasource to this warehouse Parameters: ds - ( DataSource ) The datasource object to add skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks add_dimension ( self , dimension , force=False ) Add a reference to a dimension to this FieldManager add_metric ( self , metric , force=False ) Add a reference to a metric to this FieldManager apply_config ( self , config , skip_integrity_checks=False ) Apply a warehouse config Parameters: config - ( dict ) A dict adhering to the WarehouseConfigSchema skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks delete ( id ) Delete a saved warehouse. Note that this does not delete any report specs that reference this warehouse ID. Parameters: id - ( int ) The ID of a Warehouse to delete delete_report ( self , spec_id ) Delete a report by spec ID Parameters: spec_id - ( int ) The ID of a report spec to delete directly_has_dimension ( self , name ) Check if this FieldManager directly stores this dimension directly_has_field ( self , name ) Check if this FieldManager directly stores this field directly_has_metric ( self , name ) Check if this FieldManager directly stores this metric execute ( self , metrics=None , dimensions=None , criteria=None , row_filters=None , rollup=None , pivot=None , order_by=None , limit=None , limit_first=False , adhoc_datasources=None , allow_partial=False ) Build and execute a Report Returns: ( ReportResult ) - The result of the report execute_id ( self , spec_id , adhoc_datasources=None ) Build and execute a report from a spec ID Parameters: spec_id - ( int ) The ID of a report spec adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( ReportResult ) - The result of the report get_child_field_managers ( self ) Get a list of all datasources in this warehouse get_datasource ( self , name , adhoc_datasources=None ) Get the datasource object corresponding to this datasource name Parameters: name - ( str ) The name of the datasource adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( DataSource ) - The matching datasource object get_dimension ( self , obj , adhoc_fms=None ) Get a reference to a dimension on this FieldManager get_dimension_configs ( self , adhoc_fms=None ) Get a dict of all dimension configs supported by this FieldManager get_dimension_names ( self , adhoc_fms=None ) Get a set of dimension names supported by this FieldManager get_dimension_table_set ( self , grain , dimension_grain , adhoc_datasources=None ) Get a TableSet that can satisfy dimension table joins across this grain Parameters: grain - ( list ) A list of dimension names representing the full grain required including dimension and criteria grain dimension_grain - ( list of str ) A list of dimension names representing the requested dimensions for report grouping adhoc_datasources - ( list, optional ) A list of FieldManagers for this request Returns: ( TableSet ) - A TableSet that can satisfy this request get_dimensions ( self , adhoc_fms=None ) Get a dict of all dimensions supported by this FieldManager get_direct_dimension_configs ( self ) Get a dict of dimension configs directly supported by this FieldManager get_direct_dimensions ( self ) Get dimensions directly stored on this FieldManager get_direct_fields ( self ) Get a dict of all fields directly supported by this FieldManager get_direct_metric_configs ( self ) Get a dict of metric configs directly supported by this FieldManager get_direct_metrics ( self ) Get metrics directly stored on this FieldManager get_field ( self , obj , adhoc_fms=None ) Get a refence to a field on this FieldManager get_field_instances ( self , field , adhoc_fms=None ) Get a dict of FieldManagers (including child and adhoc FMs) that support a field get_field_managers ( self , adhoc_fms=None ) Get a list of all child FieldManagers including adhoc get_field_names ( self , adhoc_fms=None ) Get a set of field names supported by this FieldManager get_fields ( self , adhoc_fms=None ) Get a dict of all fields supported by this FieldManager get_metric ( self , obj , adhoc_fms=None ) Get a reference to a metric on this FieldManager. If the object passed is a dict it is expected to define an AdHocMetric. get_metric_configs ( self , adhoc_fms=None ) Get a dict of all metric configs supported by this FieldManager get_metric_names ( self , adhoc_fms=None ) Get a set of metric names supported by this FieldManager get_metric_table_set ( self , metric , grain , dimension_grain , adhoc_datasources=None ) Get a TableSet that can satisfy a metric at a given grain Parameters: metric - ( str ) A metric name grain - ( list ) A list of dimension names representing the full grain required including dimension and criteria grain dimension_grain - ( list of str ) A list of dimension names representing the requested dimensions for report grouping adhoc_datasources - ( list, optional ) A list of FieldManagers for this request Returns: ( TableSet ) - A TableSet that can satisfy this request get_metrics ( self , adhoc_fms=None ) Get a dict of all metrics supported by this FieldManager has_dimension ( self , name , adhoc_fms=None ) Check whether a dimension is contained in this FieldManager has_field ( self , name , adhoc_fms=None ) Check whether a field is contained in this FieldManager has_metric ( self , name , adhoc_fms=None ) Check whether a metric is contained in this FieldManager load ( id ) Load a Warehouse from a Warehouse ID Parameters: id - ( int ) A Warehouse ID Returns: ( Warehouse ) - A Warehouse object load_report ( self , spec_id , adhoc_datasources=None ) Load a report from a spec ID Parameters: spec_id - ( int ) The ID of a report spec adhoc_datasources - ( list, optional ) A list of FieldManagers specific to this request Returns: ( Report ) - A report built from this report spec load_report_and_warehouse ( spec_id ) Load a Report and Warehouse from a ReportSpec. The Warehouse will be populated on the returned Report object. Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( Report ) - A Report built from this report spec load_warehouse_for_report ( spec_id ) Load the warehouse corresponding to the ReportSpec Parameters: spec_id - ( int ) A ReportSpec ID Returns: ( Warehouse ) - A Warehouse object print_dimensions ( self , indent=None ) Print all dimensions in this FieldManager print_info ( self ) Print the warehouse structure print_metrics ( self , indent=None ) Print all metrics in this FieldManager remove_datasource ( self , ds , skip_integrity_checks=False ) Remove a datasource from this config Parameters: ds - ( DataSource ) The datasource object to remove skip_integrity_checks - ( bool, optional ) If True, skip warehouse integrity checks run_integrity_checks ( self , adhoc_datasources=None ) Run a series of integrity checks on the warehouse and its datasources. This will raise a WarehouseException with all failed checks. Parameters: adhoc_datasources - ( list, optional ) A list of FieldManagers to include for this request save ( self , name , config_url , meta=None ) Save the warehouse config and return the ID Parameters: name - ( str ) A name to give the Warehouse config_url - ( str ) A URL pointing to a config file that can be used to recreate the warehouse meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the warehouse Returns: ( int ) - The ID of the saved Warehouse save_report ( self , meta=None , **kwargs ) Init a Report and save it as a ReportSpec. Note that the Warehouse must be saved before any ReportSpecs can be saved for the Warehouse. Parameters: meta - ( object, optional ) A metadata object to be serialized as JSON and stored with the report **kwargs - Passed through to Report Returns: ( Report ) - The built report with the spec ID populated","title":"Warehouse"}]}